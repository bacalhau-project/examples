{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "sidebar_label: \"From S3\"\n",
        "sidebar_position: 3\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Copy Data from S3 to IPFS\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bacalhau-project/examples/blob/main/data-ingestion/s3-to-ipfs/index.ipynb)\n",
        "[![Open In Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/bacalhau-project/examples/HEAD?labpath=data-ingestion/s3-to-ipfs/index.ipynb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RAeGPyKPLl-n"
      },
      "source": [
        "\n",
        "In this tutorial, to copy Data from S3 to IPFS, we will scrape all the links from a public AWS S3 buckets and then copy the data to IPFS using Bacalhau. \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisite\n",
        "\n",
        "To get started, you need to install the Bacalhau client, see more information [here](https://docs.bacalhau.org/getting-started/installation)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running a Bacalhau Job\n",
        "\n",
        "If your bucket has more than 1000 files, with the command below, you can submit a Bacalhau job to extract the URL list of the files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9W0W8nQLbqc",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "%%bash --out job_id\n",
        "bacalhau docker run \\\n",
        "-u https://noaa-goes16.s3.amazonaws.com/ \\\n",
        "-v QmR1qXs8Y8T7G6F2Yy91sDTWG6WAhoFrCjMGRvy7N1y5LC:/extract.py \\\n",
        "--id-only \\\n",
        "--wait \\\n",
        "python \\\n",
        "-- /bin/bash -c 'python3 extract.py https://noaa-goes16.s3.amazonaws.com/  /inputs'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Structure of the Command\n",
        "\n",
        "Let's look closely at the command above:\n",
        "\n",
        "- `bacalhau docker run`: call to bacalhau \n",
        "\n",
        "- `-u  https://noaa-goes16.s3.amazonaws.com/`: the name of the bucket we want to extract URLs from. Repeplace the placeholders with `noaa-goes16` which your own name.\n",
        "\n",
        "-  `-v QmR1qXs8Y8T7G6F2Yy91sDTWG6WAhoFrCjMGRvy7N1y5LC:/extract.py \\`: Mounting the scrapper script CID, this script extracts the links from the XML document tree\n",
        "\n",
        "- `-- /bin/bash -c 'python3 extract.py https://noaa-goes16.s3.amazonaws.com/  /inputs'`: Executing the scrapper script path to input dataset\n",
        "\n",
        "\n",
        "The command above extracts the path of the file in the bucket, we added the URL as a prefix to the path `https://noaa-goes16.s3.amazonaws.com/`  then provided the path where the XML document tree of the URL is mounted which is `/inputs`\n",
        "\n",
        "\n",
        "When a job is sumbitted, Bacalhau prints out the related `job_id`. We store that in an environment variable so that we can reuse it later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gg6riPesa5BN",
        "outputId": "c4de89b0-fa8c-4231-b712-da30a5e001e0",
        "tags": [
          "skip-execution",
          "remove_cell"
        ]
      },
      "outputs": [],
      "source": [
        "%%env JOB_ID={job_id}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::tip\n",
        "This only works with datasets that are publicly accessible and don't require an AWS account or pay to use buckets and possibly only limited to first 1000 URLs.\n",
        ":::"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Checking the State of your Jobs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Job status**: You can check the status of the job using `bacalhau list`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0pc73fqa5BO",
        "outputId": "2a690ede-3096-4b9e-8131-004052ab6241",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "bacalhau list --id-filter ${JOB_ID} --wide"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When it says `Published` or `Completed`, that means the job is done, and we can get the results."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Job information**: You can find out more information about your job by using `bacalhau describe`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRgrNAm7a5BO",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "bacalhau describe ${JOB_ID}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Job download**: You can download your job results directly by using `bacalhau get`. Alternatively, you can choose to create a directory to store your results. In the command below, we created a directory and downloaded our job output to be stored in that directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duvEDPr-a5BO",
        "outputId": "db31249d-b228-4c74-b4b3-11684ef0dca0",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "rm -rf results && mkdir -p results # Temporary directory to store the results\n",
        "bacalhau get $JOB_ID --output-dir results # Download the results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YZEeFzMEa5BO"
      },
      "source": [
        "After the download has finished you should see the following contents in results directory."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Viewing your Job Output\n",
        "\n",
        "Each job creates 3 subfolders: the **combined_results**, **per_shard files**, and the **raw** directory. To view your file, run the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhk8g_LMa5BO",
        "outputId": "b90b2c8f-9f46-4191-a30c-84d7faadb0b7",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://noaa-goes16.s3.amazonaws.com/ABI-L1b-RadC/2000/001/12/OR_ABI-L1b-RadC-M3C01_G16_s20000011200000_e20000011200000_c20170671748180.nc\n",
            "https://noaa-goes16.s3.amazonaws.com/ABI-L1b-RadC/2000/001/12/OR_ABI-L1b-RadC-M3C01_G16_s20000011200000_e20000011200000_c20170691603180.nc\n",
            "https://noaa-goes16.s3.amazonaws.com/ABI-L1b-RadC/2000/001/12/OR_ABI-L1b-RadC-M3C01_G16_s20000011200000_e20000011200000_c20170751219598.nc\n",
            "https://noaa-goes16.s3.amazonaws.com/ABI-L1b-RadC/2000/001/12/OR_ABI-L1b-RadC-M3C01_G16_s20000011200000_e20000011200000_c20170752149454.nc\n",
            "https://noaa-goes16.s3.amazonaws.com/ABI-L1b-RadC/2000/001/12/OR_ABI-L1b-RadC-M3C01_G16_s20000011200000_e20000011200000_c20170752204183.nc\n",
            "https://noaa-goes16.s3.amazonaws.com/ABI-L1b-RadC/2000/001/12/OR_ABI-L1b-RadC-M3C01_G16_s20000011200000_e20000011200000_c20170752234173.nc\n",
            "https://noaa-goes16.s3.amazonaws.com/ABI-L1b-RadC/2000/001/12/OR_ABI-L1b-RadC-M3C01_G16_s20000011200000_e20000011200000_c20170901216521.nc\n",
            "https://noaa-goes16.s3.amazonaws.com/ABI-L1b-RadC/2000/001/12/OR_ABI-L1b-RadC-M3C01_G16_s20000011200000_e20000011200000_c20170951807462.nc\n",
            "https://noaa-goes16.s3.amazonaws.com/ABI-L1b-RadC/2000/001/12/OR_ABI-L1b-RadC-M3C01_G16_s20000011200000_e20000011200000_c20171000619157.nc\n",
            "https://noaa-goes16.s3.amazonaws.com/ABI-L1b-RadC/2000/001/12/OR_ABI-L1b-RadC-M3C01_G16_s20000011200000_e20000011200000_c20171061215161.nc\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "head -10 results/combined_results/stdout"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tjwNoT03SJPj"
      },
      "source": [
        "### Extracting Links from Job Output\n",
        "\n",
        "From the output of the job we ran above, we extracted the links that we want.next is to save them to IPFS using Bacalhau."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scDMtUmWqAlu"
      },
      "source": [
        "Selecting the first ten links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mx0lWV4lS7eh",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "head -10 results/combined_results/stdout > links.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Selecting all the links\n",
        "\n",
        "```\n",
        "cat results/combined_results/stdout > links.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLVSRhzNqGiW"
      },
      "source": [
        "Creating a script to submit jobs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRFgBkvtRwok",
        "outputId": "b4bafd1c-7f9a-4e3b-823e-5143a73ccf04",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "%%writefile move.sh\n",
        "#!/usr/bin/env bash\n",
        "while read URL; do\n",
        "  bacalhau docker run --input-urls=\"${URL}\" \\\n",
        "  --id-only \\\n",
        "  --wait \\\n",
        "  docker.io/bacalhauproject/uploader:v0.9.14\n",
        "done < links.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaSZoXfVqQ0d"
      },
      "source": [
        "Running the script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPe85SIQSd-R",
        "outputId": "e79185ab-9a8d-4634-f12d-03485bcb5c49",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c5c0b6dd-ce86-4b19-b666-43e3ed6fb0b4\n",
            "0a599b27-3063-46a4-82ae-244e653e0187\n",
            "2c8b7427-ee96-49b4-9516-c8596669b15f\n",
            "2cd130c1-c007-4715-a3e5-6c2d81456c09\n",
            "8c68e7be-5f85-4f2e-9cb8-3c2bb91748ae\n",
            "2850f638-6541-4ee4-9c4a-9d650699671f\n",
            "d6fb611c-a5c8-4515-9fae-53f7c7a0cfec\n",
            "6e453d0e-0baf-4905-9fa8-5ce54e5d4b65\n",
            "8177fe99-920d-4410-9cc6-bd9d0bf70f8e\n",
            "9c1acb25-6fec-4d14-a91a-4a1f60f985b9\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "bash move.sh"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rmb3HdENqTns"
      },
      "source": [
        "### List the Outputs of the Jobs in JSON Format\n",
        "\n",
        "In this case, we will move the first 10 URLs and set the no of jobs to 10 `-n 10`. If you have submitted the whole list you can set `-n` to 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHx8A0IQSzVK",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "bacalhau list -n 10 --output json > output.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92bE68fpqZgb"
      },
      "source": [
        "Installing jq to extract CID from the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivH6cRImnsF4",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "sudo apt update\n",
        "sudo apt install jq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_6kufThqlzT"
      },
      "source": [
        "Extracting the CIDs from output json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvZf1k-9g6e7",
        "outputId": "b84095d0-af73-48c8-c164-de8cfd0fdc5d",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"QmV2uYcS7TqQGDvsLnoC2yn1inKoec9vVyTa548Gg6VTkr\"\n",
            "\"QmaZXQSxFDMjneyCv7ZjXdgWTNbLwPRmSEy3PMPjByeQZw\"\n",
            "\"QmQkafCQoSCevLN6hJKCJYRK67z3VEsFWk7qSq85GW9NUt\"\n",
            "\"QmZFzHeACRcqfPwTCzCfsikDLixX1NdBXCG6RHH1iiuCiY\"\n",
            "\"QmdZQ8vmzWRuzn9jVgzRxKnBhLsX1TQwvfT6QZdNDzcCsR\"\n",
            "\"QmVTL12jSTNR62zyM8zX7jVSCp1Mb5B2PUV1xkct4vo1SP\"\n",
            "\"QmaN5p8zteJ868cbmThTHd4yumB5eetWxXoLbcP4hWBzF1\"\n",
            "\"Qme3kw2tbNfmFPHXydDK9dKLzwfry8b2dxD5s4L1ij9QAL\"\n",
            "\"QmYki5KZQHroo1zzYWfPYrnNRDec8MVjkrvSRBCQqMzvHY\"\n",
            "\"QmNjarM2oxMPwN4cpQcy6NhuNbe4opHyfdce149oYkasjG\"\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "jq '.[] .\"JobState\" .\"Nodes\"' output.json > output-shards.json\n",
        "jq '.[].\"Shards\".\"0\".\"PublishedResults\".\"CID\" | select( . != null )'  output-shards.json"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Need Support?\n",
        "\n",
        "For questions, feedback, please reach out in our [forum](https://github.com/filecoin-project/bacalhau/discussions)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10 (default, Jun 22 2022, 20:18:18) \n[GCC 9.4.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
