{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "sidebar_label: \"s3-to-ipfs\"\n",
        "sidebar_position: 3\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Copy Data from S3 to IPFS\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bacalhau-project/examples/blob/main/data-ingestion/s3-to-ipfs/index.ipynb)\n",
        "[![Open In Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/bacalhau-project/examples/HEAD?labpath=data-ingestion/s3-to-ipfs/index.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAeGPyKPLl-n"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this example we will scrape all the links from public s3 buckets and copy the data from s3 to IPFS.\n",
        "It works by Extracting paths of files from the document tree, these links can be later be used to download the content of a s3 bucket and later we will use a shell script to submit bacalhau jobs that copy the data to IPFS\n",
        "\n",
        "By following this example you can move the datasets you want from s3 to IPFS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0-81nSiM3ty"
      },
      "source": [
        "## Getting the URL of files in the bucket\n",
        "\n",
        "Note:-There are certain limitations to this step, which only works with datasets that are publicly accessible and don't require an AWS account or pay to use buckets and possibly only limited to first 1000 URLs, you can pass in your own URL list of all the files\n",
        "that you want to copy if the bucket has more than 1000 files\n",
        "\n",
        "Structure of the command\n",
        "\n",
        "```\n",
        "bacalhau docker run \\\n",
        "-u https://<name-of-the-bucket>.s3.amazonaws.com \\\n",
        "-v QmR1qXs8Y8T7G6F2Yy91sDTWG6WAhoFrCjMGRvy7N1y5LC:/extract.py \\\n",
        "python \\\n",
        "-- python3 extract.py https://<name-of-the-bucket>.s3.amazonaws.com/  /inputs\n",
        "```\n",
        "\n",
        "replace <name-of-the-bucket> with the name of the bucket you want to extract the URLs from\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Rt_B9bjEPlC",
        "outputId": "4a306986-1f1d-40e1-bb32-cf6a83c2d317",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your system is linux_amd64\n",
            "No BACALHAU detected. Installing fresh BACALHAU CLI...\n",
            "Getting the latest BACALHAU CLI...\n",
            "Installing v0.3.11 BACALHAU CLI...\n",
            "Downloading https://github.com/filecoin-project/bacalhau/releases/download/v0.3.11/bacalhau_v0.3.11_linux_amd64.tar.gz ...\n",
            "Downloading sig file https://github.com/filecoin-project/bacalhau/releases/download/v0.3.11/bacalhau_v0.3.11_linux_amd64.tar.gz.signature.sha256 ...\n",
            "Verified OK\n",
            "Extracting tarball ...\n",
            "NOT verifying Bin\n",
            "bacalhau installed into /usr/local/bin successfully.\n",
            "Client Version: v0.3.11\n",
            "Server Version: v0.3.11\n"
          ]
        }
      ],
      "source": [
        "!curl -sL https://get.bacalhau.org/install.sh | bash"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " Command with all the place holders replaced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9W0W8nQLbqc",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "%%bash --out job_id\n",
        "bacalhau docker run \\\n",
        "-u https://noaa-goes16.s3.amazonaws.com/ \\\n",
        "-v QmR1qXs8Y8T7G6F2Yy91sDTWG6WAhoFrCjMGRvy7N1y5LC:/extract.py \\\n",
        "--id-only \\\n",
        "--wait \\\n",
        "python \\\n",
        "-- /bin/bash -c 'python3 extract.py https://noaa-goes16.s3.amazonaws.com/  /inputs'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Structure of the command\n",
        "\n",
        "-u https://noaa-goes16.s3.amazonaws.com/\n",
        "we replace the placeholders with `noaa-goes16` which is the name of the bucket we want to extract URLs from\n",
        "\n",
        "-v QmR1qXs8Y8T7G6F2Yy91sDTWG6WAhoFrCjMGRvy7N1y5LC:/extract.py \\\n",
        "Mounting the scrapper script, this script extracts the links from the XML document tree\n",
        "\n",
        "\n",
        "-- /bin/bash -c 'python3 extract.py https://noaa-goes16.s3.amazonaws.com/  /inputs'\n",
        "Executing the scrapper script\n",
        "\n",
        "since the script extracts the path of the file in the bucket we need add the URL as a prefix to the path `https://noaa-goes16.s3.amazonaws.com/` \n",
        "\n",
        "then we provide the path where the XML document tree of the URL is mounted which is `/inputs`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gg6riPesa5BN",
        "outputId": "c4de89b0-fa8c-4231-b712-da30a5e001e0",
        "tags": [
          "skip-execution",
          "remove_cell"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: JOB_ID=12e1b4d9-00b0-4824-bbd1-6d75083dcae0\n"
          ]
        }
      ],
      "source": [
        "%env JOB_ID={job_id}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0pc73fqa5BO",
        "outputId": "2a690ede-3096-4b9e-8131-004052ab6241",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[92;100m CREATED           \u001b[0m\u001b[92;100m ID                                   \u001b[0m\u001b[92;100m JOB                                                                                          \u001b[0m\u001b[92;100m STATE     \u001b[0m\u001b[92;100m VERIFIED \u001b[0m\u001b[92;100m PUBLISHED                                            \u001b[0m\n",
            "\u001b[97;40m 22-11-13-13:52:12 \u001b[0m\u001b[97;40m 12e1b4d9-00b0-4824-bbd1-6d75083dcae0 \u001b[0m\u001b[97;40m Docker python /bin/bash -c python3 extract.py https://noaa-goes16.s3.amazonaws.com/  /inputs \u001b[0m\u001b[97;40m Completed \u001b[0m\u001b[97;40m          \u001b[0m\u001b[97;40m /ipfs/QmaxiCCJ5vuwEfA2x7VVvMUXHxHN6iYNPhmvFhXSyUyNYx \u001b[0m\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "bacalhau list --id-filter ${JOB_ID} --wide"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wElIl4UYa5BO"
      },
      "source": [
        "Where it says \"Completed\", that means the job is done, and we can get the results.\n",
        "\n",
        "To find out more information about your job, run the following command:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRgrNAm7a5BO",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "bacalhau describe ${JOB_ID}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duvEDPr-a5BO",
        "outputId": "db31249d-b228-4c74-b4b3-11684ef0dca0",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching results of job '12e1b4d9-00b0-4824-bbd1-6d75083dcae0'...\n",
            "Results for job '12e1b4d9-00b0-4824-bbd1-6d75083dcae0' have been written to...\n",
            "results\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022/11/13 13:53:09 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 2048 kiB, got: 416 kiB). See https://github.com/lucas-clemente/quic-go/wiki/UDP-Receive-Buffer-Size for details.\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "rm -rf results && mkdir -p results\n",
        "bacalhau get $JOB_ID --output-dir results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZEeFzMEa5BO"
      },
      "source": [
        "Viewing the outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhk8g_LMa5BO",
        "outputId": "b90b2c8f-9f46-4191-a30c-84d7faadb0b7",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://noaa-goes16.s3.amazonaws.com/ABI-L1b-RadC/2000/001/12/OR_ABI-L1b-RadC-M3C01_G16_s20000011200000_e20000011200000_c20170671748180.nc\n",
            "https://noaa-goes16.s3.amazonaws.com/ABI-L1b-RadC/2000/001/12/OR_ABI-L1b-RadC-M3C01_G16_s20000011200000_e20000011200000_c20170691603180.nc\n",
            "https://noaa-goes16.s3.amazonaws.com/ABI-L1b-RadC/2000/001/12/OR_ABI-L1b-RadC-M3C01_G16_s20000011200000_e20000011200000_c20170751219598.nc\n",
            "https://noaa-goes16.s3.amazonaws.com/ABI-L1b-RadC/2000/001/12/OR_ABI-L1b-RadC-M3C01_G16_s20000011200000_e20000011200000_c20170752149454.nc\n",
            "https://noaa-goes16.s3.amazonaws.com/ABI-L1b-RadC/2000/001/12/OR_ABI-L1b-RadC-M3C01_G16_s20000011200000_e20000011200000_c20170752204183.nc\n",
            "https://noaa-goes16.s3.amazonaws.com/ABI-L1b-RadC/2000/001/12/OR_ABI-L1b-RadC-M3C01_G16_s20000011200000_e20000011200000_c20170752234173.nc\n",
            "https://noaa-goes16.s3.amazonaws.com/ABI-L1b-RadC/2000/001/12/OR_ABI-L1b-RadC-M3C01_G16_s20000011200000_e20000011200000_c20170901216521.nc\n",
            "https://noaa-goes16.s3.amazonaws.com/ABI-L1b-RadC/2000/001/12/OR_ABI-L1b-RadC-M3C01_G16_s20000011200000_e20000011200000_c20170951807462.nc\n",
            "https://noaa-goes16.s3.amazonaws.com/ABI-L1b-RadC/2000/001/12/OR_ABI-L1b-RadC-M3C01_G16_s20000011200000_e20000011200000_c20171000619157.nc\n",
            "https://noaa-goes16.s3.amazonaws.com/ABI-L1b-RadC/2000/001/12/OR_ABI-L1b-RadC-M3C01_G16_s20000011200000_e20000011200000_c20171061215161.nc\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "head -10 results/combined_results/stdout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjwNoT03SJPj"
      },
      "source": [
        "## Copying the data from s3 to IPFS\n",
        "\n",
        "In this section we will just copy the first ten links that we got as a output from the previous job and save them to IPFS using bacalhau just to save time but you can select all the links"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scDMtUmWqAlu"
      },
      "source": [
        "selecting the first ten links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mx0lWV4lS7eh",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "head -10 results/combined_results/stdout > links.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "selecting all the links\n",
        "\n",
        "```\n",
        "cat results/combined_results/stdout > links.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLVSRhzNqGiW"
      },
      "source": [
        "Creating a script to submit jobs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRFgBkvtRwok",
        "outputId": "b4bafd1c-7f9a-4e3b-823e-5143a73ccf04",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting move.sh\n"
          ]
        }
      ],
      "source": [
        "%%writefile move.sh\n",
        "#!/usr/bin/env bash\n",
        "while read URL; do\n",
        "  bacalhau docker run --input-urls=\"${URL}\" \\\n",
        "  --id-only \\\n",
        "  --wait \\\n",
        "  docker.io/bacalhauproject/uploader:v0.9.14\n",
        "done < links.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaSZoXfVqQ0d"
      },
      "source": [
        "Running the script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPe85SIQSd-R",
        "outputId": "e79185ab-9a8d-4634-f12d-03485bcb5c49",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c5c0b6dd-ce86-4b19-b666-43e3ed6fb0b4\n",
            "0a599b27-3063-46a4-82ae-244e653e0187\n",
            "2c8b7427-ee96-49b4-9516-c8596669b15f\n",
            "2cd130c1-c007-4715-a3e5-6c2d81456c09\n",
            "8c68e7be-5f85-4f2e-9cb8-3c2bb91748ae\n",
            "2850f638-6541-4ee4-9c4a-9d650699671f\n",
            "d6fb611c-a5c8-4515-9fae-53f7c7a0cfec\n",
            "6e453d0e-0baf-4905-9fa8-5ce54e5d4b65\n",
            "8177fe99-920d-4410-9cc6-bd9d0bf70f8e\n",
            "9c1acb25-6fec-4d14-a91a-4a1f60f985b9\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "bash move.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmb3HdENqTns"
      },
      "source": [
        "List the outputs of the jobs in json format\n",
        "\n",
        "since in this case we just move the first 10 URLs we set the no of jobs to list to 10\n",
        "\n",
        "-n 10 but if you have submitted the whole list you can set -n to 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHx8A0IQSzVK",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "bacalhau list -n 10 --output json > output.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92bE68fpqZgb"
      },
      "source": [
        "Installing jq to extract CID from the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivH6cRImnsF4",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "sudo apt update\n",
        "sudo apt install jq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_6kufThqlzT"
      },
      "source": [
        "Extracting the CIDs from output json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvZf1k-9g6e7",
        "outputId": "b84095d0-af73-48c8-c164-de8cfd0fdc5d",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"QmV2uYcS7TqQGDvsLnoC2yn1inKoec9vVyTa548Gg6VTkr\"\n",
            "\"QmaZXQSxFDMjneyCv7ZjXdgWTNbLwPRmSEy3PMPjByeQZw\"\n",
            "\"QmQkafCQoSCevLN6hJKCJYRK67z3VEsFWk7qSq85GW9NUt\"\n",
            "\"QmZFzHeACRcqfPwTCzCfsikDLixX1NdBXCG6RHH1iiuCiY\"\n",
            "\"QmdZQ8vmzWRuzn9jVgzRxKnBhLsX1TQwvfT6QZdNDzcCsR\"\n",
            "\"QmVTL12jSTNR62zyM8zX7jVSCp1Mb5B2PUV1xkct4vo1SP\"\n",
            "\"QmaN5p8zteJ868cbmThTHd4yumB5eetWxXoLbcP4hWBzF1\"\n",
            "\"Qme3kw2tbNfmFPHXydDK9dKLzwfry8b2dxD5s4L1ij9QAL\"\n",
            "\"QmYki5KZQHroo1zzYWfPYrnNRDec8MVjkrvSRBCQqMzvHY\"\n",
            "\"QmNjarM2oxMPwN4cpQcy6NhuNbe4opHyfdce149oYkasjG\"\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "jq '.[] .\"JobState\" .\"Nodes\"' output.json > output-shards.json\n",
        "jq '.[].\"Shards\".\"0\".\"PublishedResults\".\"CID\" | select( . != null )'  output-shards.json"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
