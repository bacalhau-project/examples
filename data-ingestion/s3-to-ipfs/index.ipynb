{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "sidebar_label: \"From S3\"\n",
        "sidebar_position: 3\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Copy Data from S3 to IPFS\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bacalhau-project/examples/blob/main/data-ingestion/s3-to-ipfs/index.ipynb)\n",
        "[![Open In Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/bacalhau-project/examples/HEAD?labpath=data-ingestion/s3-to-ipfs/index.ipynb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RAeGPyKPLl-n"
      },
      "source": [
        "\n",
        "In this tutorial, to copy Data from S3 to IPFS, we will scrape all the links from a public AWS S3 buckets and then copy the data to IPFS using Bacalhau. \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisite\n",
        "\n",
        "To get started, you need to install the Bacalhau client, see more information [here](https://docs.bacalhau.org/getting-started/installation)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Getting the URLs from AWS S3 bucket\n",
        "\n",
        "If your bucket has more than 1000 files, with the command below, you can submit a Bacalhau job to extract the URL list of the files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9W0W8nQLbqc",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "%%bash --out job_id\n",
        "bacalhau docker run \\\n",
        "-u https://noaa-goes16.s3.amazonaws.com/ \\\n",
        "-v QmR1qXs8Y8T7G6F2Yy91sDTWG6WAhoFrCjMGRvy7N1y5LC:/extract.py \\\n",
        "--id-only \\\n",
        "--wait \\\n",
        "python \\\n",
        "-- /bin/bash -c 'python3 extract.py https://noaa-goes16.s3.amazonaws.com/  /inputs'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Before running the command above, replace the following:\n",
        "\n",
        "- `-u  https://noaa-goes16.s3.amazonaws.com/`: we replace the placeholders with `noaa-goes16` which is the name of the bucket we want to extract URLs from\n",
        "\n",
        "-  `-v QmR1qXs8Y8T7G6F2Yy91sDTWG6WAhoFrCjMGRvy7N1y5LC:/extract.py \\`: Mounting the scrapper script, this script extracts the links from the XML document tree\n",
        "\n",
        " - `-- /bin/bash -c 'python3 extract.py https://noaa-goes16.s3.amazonaws.com/  /inputs'`: Executing the scrapper script\n",
        "\n",
        "The command above extracts the path of the file in the bucket, we added the URL as a prefix to the path `https://noaa-goes16.s3.amazonaws.com/`  then provided the path where the XML document tree of the URL is mounted which is `/inputs`\n",
        "\n",
        "When a job is sumbitted, Bacalhau prints out the related `job_id`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gg6riPesa5BN",
        "outputId": "c4de89b0-fa8c-4231-b712-da30a5e001e0",
        "tags": [
          "skip-execution",
          "remove_cell"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: JOB_ID=12e1b4d9-00b0-4824-bbd1-6d75083dcae0\n"
          ]
        }
      ],
      "source": [
        "%env JOB_ID={job_id}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::note\n",
        "There are certain limitations to this step, as this only works with datasets that are publicly accessible and don't require an AWS account or pay to use buckets and possibly only limited to first 1000 URLs.\n",
        ":::"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Checking the State of your Jobs\n",
        " \n",
        "Now we're ready to submit a Bacalhau job. Below is the command with the place holders that should replaced."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Job status**: You can check the status of the job using `bacalhau list`. The command shows the status of our job with the with the output we  with the following command. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0pc73fqa5BO",
        "outputId": "2a690ede-3096-4b9e-8131-004052ab6241",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[92;100m CREATED           \u001b[0m\u001b[92;100m ID                                   \u001b[0m\u001b[92;100m JOB                                                                                          \u001b[0m\u001b[92;100m STATE     \u001b[0m\u001b[92;100m VERIFIED \u001b[0m\u001b[92;100m PUBLISHED                                            \u001b[0m\n",
            "\u001b[97;40m 22-11-13-13:52:12 \u001b[0m\u001b[97;40m 12e1b4d9-00b0-4824-bbd1-6d75083dcae0 \u001b[0m\u001b[97;40m Docker python /bin/bash -c python3 extract.py https://noaa-goes16.s3.amazonaws.com/  /inputs \u001b[0m\u001b[97;40m Completed \u001b[0m\u001b[97;40m          \u001b[0m\u001b[97;40m /ipfs/QmaxiCCJ5vuwEfA2x7VVvMUXHxHN6iYNPhmvFhXSyUyNYx \u001b[0m\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "bacalhau list --id-filter ${JOB_ID} --wide"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When it says `Published` or `Completed`, that means the job is done, and we can get the results."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Job information**: To find out more information about your job, run the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRgrNAm7a5BO",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "bacalhau describe ${JOB_ID}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Job download**: You can download your job results directly by using `bacalhau get`. Alternatively, you can choose to create a directory to store your results. \n",
        "\n",
        "In the command below, we created a directory and downloaded our job output to be stored in that directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duvEDPr-a5BO",
        "outputId": "db31249d-b228-4c74-b4b3-11684ef0dca0",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching results of job '12e1b4d9-00b0-4824-bbd1-6d75083dcae0'...\n",
            "Results for job '12e1b4d9-00b0-4824-bbd1-6d75083dcae0' have been written to...\n",
            "results\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022/11/13 13:53:09 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 2048 kiB, got: 416 kiB). See https://github.com/lucas-clemente/quic-go/wiki/UDP-Receive-Buffer-Size for details.\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "rm -rf results && mkdir -p results\n",
        "bacalhau get $JOB_ID --output-dir results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YZEeFzMEa5BO"
      },
      "source": [
        "After the download has finished you should see the following contents in results directory."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Viewing your Job Output\n",
        "\n",
        "Each job creates 3 subfolders: the **combined_results**,**per_shard files**, and the **raw** directory.\n",
        "\n",
        "In each of these sub_folders, you'll find the **studout** and **stderr** file.\n",
        "\n",
        "To view the file in the stdout folder, run the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhk8g_LMa5BO",
        "outputId": "b90b2c8f-9f46-4191-a30c-84d7faadb0b7",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://noaa-goes16.s3.amazonaws.com/ABI-L1b-RadC/2000/001/12/OR_ABI-L1b-RadC-M3C01_G16_s20000011200000_e20000011200000_c20170671748180.nc\n",
            "https://noaa-goes16.s3.amazonaws.com/ABI-L1b-RadC/2000/001/12/OR_ABI-L1b-RadC-M3C01_G16_s20000011200000_e20000011200000_c20170691603180.nc\n",
            "https://noaa-goes16.s3.amazonaws.com/ABI-L1b-RadC/2000/001/12/OR_ABI-L1b-RadC-M3C01_G16_s20000011200000_e20000011200000_c20170751219598.nc\n",
            "https://noaa-goes16.s3.amazonaws.com/ABI-L1b-RadC/2000/001/12/OR_ABI-L1b-RadC-M3C01_G16_s20000011200000_e20000011200000_c20170752149454.nc\n",
            "https://noaa-goes16.s3.amazonaws.com/ABI-L1b-RadC/2000/001/12/OR_ABI-L1b-RadC-M3C01_G16_s20000011200000_e20000011200000_c20170752204183.nc\n",
            "https://noaa-goes16.s3.amazonaws.com/ABI-L1b-RadC/2000/001/12/OR_ABI-L1b-RadC-M3C01_G16_s20000011200000_e20000011200000_c20170752234173.nc\n",
            "https://noaa-goes16.s3.amazonaws.com/ABI-L1b-RadC/2000/001/12/OR_ABI-L1b-RadC-M3C01_G16_s20000011200000_e20000011200000_c20170901216521.nc\n",
            "https://noaa-goes16.s3.amazonaws.com/ABI-L1b-RadC/2000/001/12/OR_ABI-L1b-RadC-M3C01_G16_s20000011200000_e20000011200000_c20170951807462.nc\n",
            "https://noaa-goes16.s3.amazonaws.com/ABI-L1b-RadC/2000/001/12/OR_ABI-L1b-RadC-M3C01_G16_s20000011200000_e20000011200000_c20171000619157.nc\n",
            "https://noaa-goes16.s3.amazonaws.com/ABI-L1b-RadC/2000/001/12/OR_ABI-L1b-RadC-M3C01_G16_s20000011200000_e20000011200000_c20171061215161.nc\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "head -10 results/combined_results/stdout"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tjwNoT03SJPj"
      },
      "source": [
        "## Copying the data from AWS S3 to IPFS\n",
        "\n",
        "From the output of the job we ran above, we extracted the links that we want.next is to save them to IPFS using Bacalhau."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scDMtUmWqAlu"
      },
      "source": [
        "Selecting the first ten links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mx0lWV4lS7eh",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "head -10 results/combined_results/stdout > links.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Selecting all the links\n",
        "\n",
        "```\n",
        "cat results/combined_results/stdout > links.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLVSRhzNqGiW"
      },
      "source": [
        "Creating a script to submit jobs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRFgBkvtRwok",
        "outputId": "b4bafd1c-7f9a-4e3b-823e-5143a73ccf04",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting move.sh\n"
          ]
        }
      ],
      "source": [
        "%%writefile move.sh\n",
        "#!/usr/bin/env bash\n",
        "while read URL; do\n",
        "  bacalhau docker run --input-urls=\"${URL}\" \\\n",
        "  --id-only \\\n",
        "  --wait \\\n",
        "  docker.io/bacalhauproject/uploader:v0.9.14\n",
        "done < links.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaSZoXfVqQ0d"
      },
      "source": [
        "Running the script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPe85SIQSd-R",
        "outputId": "e79185ab-9a8d-4634-f12d-03485bcb5c49",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c5c0b6dd-ce86-4b19-b666-43e3ed6fb0b4\n",
            "0a599b27-3063-46a4-82ae-244e653e0187\n",
            "2c8b7427-ee96-49b4-9516-c8596669b15f\n",
            "2cd130c1-c007-4715-a3e5-6c2d81456c09\n",
            "8c68e7be-5f85-4f2e-9cb8-3c2bb91748ae\n",
            "2850f638-6541-4ee4-9c4a-9d650699671f\n",
            "d6fb611c-a5c8-4515-9fae-53f7c7a0cfec\n",
            "6e453d0e-0baf-4905-9fa8-5ce54e5d4b65\n",
            "8177fe99-920d-4410-9cc6-bd9d0bf70f8e\n",
            "9c1acb25-6fec-4d14-a91a-4a1f60f985b9\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "bash move.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmb3HdENqTns"
      },
      "source": [
        "### List the outputs of the jobs in json format\n",
        "\n",
        "In this case, we will move the first 10 URLs and set the no of jobs to 10 `-n 10`. If you have submitted the whole list you can set `-n` to 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHx8A0IQSzVK",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "bacalhau list -n 10 --output json > output.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92bE68fpqZgb"
      },
      "source": [
        "Installing jq to extract CID from the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivH6cRImnsF4",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "sudo apt update\n",
        "sudo apt install jq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_6kufThqlzT"
      },
      "source": [
        "Extracting the CIDs from output json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvZf1k-9g6e7",
        "outputId": "b84095d0-af73-48c8-c164-de8cfd0fdc5d",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"QmV2uYcS7TqQGDvsLnoC2yn1inKoec9vVyTa548Gg6VTkr\"\n",
            "\"QmaZXQSxFDMjneyCv7ZjXdgWTNbLwPRmSEy3PMPjByeQZw\"\n",
            "\"QmQkafCQoSCevLN6hJKCJYRK67z3VEsFWk7qSq85GW9NUt\"\n",
            "\"QmZFzHeACRcqfPwTCzCfsikDLixX1NdBXCG6RHH1iiuCiY\"\n",
            "\"QmdZQ8vmzWRuzn9jVgzRxKnBhLsX1TQwvfT6QZdNDzcCsR\"\n",
            "\"QmVTL12jSTNR62zyM8zX7jVSCp1Mb5B2PUV1xkct4vo1SP\"\n",
            "\"QmaN5p8zteJ868cbmThTHd4yumB5eetWxXoLbcP4hWBzF1\"\n",
            "\"Qme3kw2tbNfmFPHXydDK9dKLzwfry8b2dxD5s4L1ij9QAL\"\n",
            "\"QmYki5KZQHroo1zzYWfPYrnNRDec8MVjkrvSRBCQqMzvHY\"\n",
            "\"QmNjarM2oxMPwN4cpQcy6NhuNbe4opHyfdce149oYkasjG\"\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "jq '.[] .\"JobState\" .\"Nodes\"' output.json > output-shards.json\n",
        "jq '.[].\"Shards\".\"0\".\"PublishedResults\".\"CID\" | select( . != null )'  output-shards.json"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.16 (default, Jan 10 2023, 15:23:34) \n[GCC 9.4.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "9ac03a0a6051494cc606d484d27d20fce22fb7b4d169f583271e11d5ba46a56e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
