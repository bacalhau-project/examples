{
    "cells": [
        {
            "cell_type": "raw",
            "metadata": {},
            "source": [
                "---\n",
                "sidebar_label: \"Data Ingestion\"\n",
                "sidebar_position: 10\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# How to Ingest Data For Use in Bacalhau\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bacalhau-project/examples/blob/main/data-ingestion/index.ipynb)\n",
                "[![Open In Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/bacalhau-project/examples/HEAD?labpath=data-ingestion/index.ipynb)\n",
                "\n",
                "Before you can start crunching data, you need to make it addressable and accessible via [IPFS](https://ipfs.io/). This notebook will demonstrate several ways to do that.\n",
                "\n",
                "### Introduction\n",
                "\n",
                "The goal of the Bacalhau project is to make it easy to perform distributed, decentralised computation next to where the data resides. So a key step in this process is making your data accessible.\n",
                "\n",
                "IPFS is a set of protocols that allow data to be discovered and accessed in a decentralised way. Data is identified by its content identifier (CID) and can be accessed by anyone who knows the CID. This notebook will show you two ways of interacting with IPFS to move your data from one place (e.g. your machine) to IPFS.\n",
                "\n",
                "### Prerequisites\n",
                "\n",
                "* The [Bacalhau CLI](https://docs.bacalhau.org/getting-started/installation) (if you want to run the Bacalhau examples)\n",
                "* [Docker](https://docs.docker.com/engine/install/) (if you want to run the Docker examples)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Moving Data via Bacalhau\n",
                "\n",
                "The easiest way to move data into IPFS is by leveraging helper functions in the Bacalhau CLI."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {
                "tags": [
                    "remove_cell"
                ]
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Your system is darwin_arm64\n",
                        "\n",
                        "BACALHAU CLI is detected:\n",
                        "Client Version: v0.3.2\n",
                        "Server Version: v0.3.2\n",
                        "Reinstalling BACALHAU CLI - ./bacalhau...\n",
                        "Getting the latest BACALHAU CLI...\n",
                        "Installing v0.3.2 BACALHAU CLI...\n",
                        "Downloading https://github.com/filecoin-project/bacalhau/releases/download/v0.3.2/bacalhau_v0.3.2_darwin_arm64.tar.gz ...\n",
                        "Downloading sig file https://github.com/filecoin-project/bacalhau/releases/download/v0.3.2/bacalhau_v0.3.2_darwin_arm64.tar.gz.signature.sha256 ...\n",
                        "Verified OK\n",
                        "Extracting tarball ...\n",
                        "NOT verifying Bin\n",
                        "bacalhau installed into . successfully.\n",
                        "Client Version: v0.3.2\n",
                        "Server Version: v0.3.2\n",
                        "env: PATH=./:./:./:./:./:/Users/phil/.pyenv/versions/3.9.7/bin:/opt/homebrew/Caskroom/google-cloud-sdk/latest/google-cloud-sdk/bin:/Users/phil/.gvm/bin:/opt/homebrew/opt/findutils/libexec/gnubin:/opt/homebrew/opt/coreutils/libexec/gnubin:/opt/homebrew/Caskroom/google-cloud-sdk/latest/google-cloud-sdk/bin:/Users/phil/.pyenv/shims:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/usr/local/MacGPG2/bin:/Users/phil/.nexustools\n"
                    ]
                }
            ],
            "source": [
                "!(export BACALHAU_INSTALL_DIR=.; curl -sL https://get.bacalhau.org/install.sh | bash)\n",
                "path=!echo $PATH\n",
                "%env PATH=./:{path[0]}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### URL -> IPFS\n",
                "\n",
                "The Bacalhau binary includes a helper function to upload from a public URL. This is useful if you have data hosted on a website or in a public S3 bucket (for example).\n",
                "\n",
                "The following code copies the data from a specified URL to the `/ouputs` directory of a Bacalhau job, and then uploads it to IPFS. Bacalhau will return the CID of the uploaded data.\n",
                "\n",
                ":::tip\n",
                "\n",
                "Be careful with the syntax of the command in this example. The `--input-urls` flag only supports writing from a single URL that represents a file to a single path that includes a file. You cannot write to a directory.\n",
                "\n",
                "To make sure, you can add an `ls` to the command to see what is exposed in the input directory and then download the result and look at the stdout.\n",
                ":::"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash --out=job_id\n",
                "bacalhau docker run \\\n",
                "    --wait \\\n",
                "    --id-only \\\n",
                "    --input-urls http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz ubuntu -- cp -rv /inputs/. /outputs/"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "env: JOB_ID=de712a10-37dc-4ceb-915d-571ad00a6bf4\n"
                    ]
                }
            ],
            "source": [
                "%env JOB_ID={job_id}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[92;100m CREATED           \u001b[0m\u001b[92;100m ID                                   \u001b[0m\u001b[92;100m JOB                                      \u001b[0m\u001b[92;100m STATE     \u001b[0m\u001b[92;100m VERIFIED \u001b[0m\u001b[92;100m PUBLISHED                                            \u001b[0m\n",
                        "\u001b[97;40m 22-10-11-10:04:01 \u001b[0m\u001b[97;40m de712a10-37dc-4ceb-915d-571ad00a6bf4 \u001b[0m\u001b[97;40m Docker ubuntu cp -rv /inputs/. /outputs/ \u001b[0m\u001b[97;40m Completed \u001b[0m\u001b[97;40m          \u001b[0m\u001b[97;40m /ipfs/Qma5e6EDpPe2TsKuz3tumSPSta6vtx48A18f9k99HJATfp \u001b[0m\n"
                    ]
                }
            ],
            "source": [
                "%%bash\n",
                "bacalhau list --id-filter ${JOB_ID} --wide"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The output of the list command presents the CID of the output directory. You can use this in subsequent jobs. For example, let's run a simple command to `ls` the contents of that CID.\n",
                "\n",
                ":::warning\n",
                "\n",
                "This file is not pinned. There is no guarantee that the file will exist in the future. If you want to ensure that the file is pinned, use a pinning service.\n",
                "\n",
                ":::"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Job successfully submitted. Job ID: 28850250-687d-440e-b6e6-fb809ead8f97\n",
                        "Checking job status... (Enter Ctrl+C to exit at any time, your job will continue running):\n",
                        "\n",
                        "\t       Creating job for submission ... done ✅\n",
                        "\t       Finding node(s) for the job ... done ✅\n",
                        "\t             Node accepted the job ... done ✅\n",
                        "\t                                   ... done ✅\n",
                        "\t   Job finished, verifying results ... done ✅\n",
                        "\t      Results accepted, publishing ... done ✅\n",
                        "\t                                  \n",
                        "Results CID: QmSTbh1wRkwcNkjTmCWjUWxwaBs1q2BtG5r2U6mere5ARc\n",
                        "Job Results By Node:\n",
                        "Node QmXaXu9N:\n",
                        "  Shard 0:\n",
                        "    Status: Cancelled\n",
                        "    No RunOutput for this shard\n",
                        "Node QmYgxZiy:\n",
                        "  Shard 0:\n",
                        "    Status: Completed\n",
                        "    Container Exit Code: 0\n",
                        "    Stdout:\n",
                        "      total 9684\n",
                        "-rw-r--r-- 1 root root 9912422 Oct 11 10:04 train-images-idx3-ubyte.gz\n",
                        "    Stderr: <NONE>\n",
                        "Node QmdZQ7Zb:\n",
                        "  Shard 0:\n",
                        "    Status: Cancelled\n",
                        "    No RunOutput for this shard\n",
                        "\n",
                        "To download the results, execute:\n",
                        "  bacalhau get 28850250-687d-440e-b6e6-fb809ead8f97\n",
                        "\n",
                        "To get more details about the run, execute:\n",
                        "  bacalhau describe 28850250-687d-440e-b6e6-fb809ead8f97\n"
                    ]
                }
            ],
            "source": [
                "%%bash\n",
                "bacalhau docker run --inputs Qma5e6EDpPe2TsKuz3tumSPSta6vtx48A18f9k99HJATfp ubuntu -- ls -l /inputs/outputs/"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Using a Third-Party to Pin Data\n",
                "\n",
                "If you have data that you want to make available to your Bacalhau jobs (or other people), you can pin it using a pinning service. Pinning services store data on behalf of users and expose the data over IPFS. The pinning provider is essentially guaranteeing that your data will be available if someone knows the CID. Implementation details differ, but the pinning services often use a combination of IPFS nodes and third-party storage providers which are paid for via cryptocurrencies like Filecoin. Most pinning services offer you a free tier, so you can try them out without spending any money."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Web3.Storage\n",
                "\n",
                "This example will demonstrate how to pin data using Web3.Storage. Web3.Storage is a pinning service that is built on top of IPFS and Filecoin. It is free to use for small amounts of data, and has a generous free tier. You can find more information about Web3.Storage [here](https://web3.storage/).\n",
                "\n",
                "#### 1. Create an Account\n",
                "\n",
                "First you need to create an account (if you don't have one already). Browse to https://web3.storage/login/ and sign up.\n",
                "\n",
                "#### 2. Sign In and Create an API Key\n",
                "\n",
                "Next, sign in and browse to the [\"Create API Key\" page](https://web3.storage/tokens/?create=true). Follow the instructions to create an API key. Once created, you will need to copy the API key to your clipboard.\n",
                "\n",
                "#### 3. Pin a Local File Using Their Test Client\n",
                "\n",
                "To test that your API key is working, use [web3.storage's test client to test that it's working](https://bafybeic5r5yxjh5xpmeczfp34ysrjcoa66pllnjgffahopzrl5yhex7d7i.ipfs.dweb.link/).\n",
                "\n",
                "You can now see (or upload) your file via the web3.storage account page: https://web3.storage/account/.\n",
                "\n",
                ":::warning\n",
                "Note that you shouldn't share your API key with anyone. Delete this API key once you have finished with this example.\n",
                ":::\n",
                "\n",
                "#### 4. Pin a Local File Via Curl\n",
                "\n",
                "You can also pin a file via curl. Please view the [API documentation](https://web3.storage/docs/reference/http-api/) to see all available commands. This example submits a single file to be pinned.\n",
                "\n",
                "```bash\n",
                "export TOKEN=YOUR_API_KEY\n",
                "echo hello world > foo.txt\n",
                "curl -X POST https://api.web3.storage/upload -H \"Authorization: Bearer ${TOKEN}\" -H \"X-NAME: foo.txt\" -d @foo.txt\n",
                "```\n",
                "\n",
                "#### 5. Pin Multiple Local Files Via Node.JS\n",
                "\n",
                "Web3.Storage has a [node.js library](https://web3.storage/docs/reference/js-client-library/) to interact with their API. The following example requires node.js to be installed. The following code uses a docker container. The javascript code is located on [their website](https://web3.storage/docs/intro/#create-the-upload-script) or on [github](https://github.com/bacalhau-project/examples/blob/main/data-ingestion/nodejs/put-files.js).\n",
                "\n",
                "First create some files to upload."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Overwriting nodejs/test1.txt\n"
                    ]
                }
            ],
            "source": [
                "%%writefile nodejs/test1.txt\n",
                "First test file"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Overwriting nodejs/test2.txt\n"
                    ]
                }
            ],
            "source": [
                "%%writefile nodejs/test2.txt\n",
                "Second test file"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Then run the following command, which uses the environmental variable `TOKEN` to authenticate with the API.\n",
                "\n",
                "```bash\n",
                "export TOKEN=YOUR_API_KEY\n",
                "docker run --rm --env TOKEN=$TOKEN -v $PWD/nodejs:/nodejs node:18-alpine ash -c 'cd /nodejs && npm install && node put-files.js --token=$TOKEN test1.txt test2.txt'\n",
                "```\n",
                "\n",
                "```\n",
                "\n",
                "up to date, audited 245 packages in 706ms\n",
                "\n",
                "54 packages are looking for funding\n",
                "  run `npm fund` for details\n",
                "\n",
                "found 0 vulnerabilities\n",
                "Uploading 2 files\n",
                "Content added with CID: bafybeic5smk3bgbsisp566kapp5clmo2ofgmvf223behdpcvjpndpnafka\n",
                "```\n",
                "\n",
                "The CID listed at the bottom can now be used as an input to Bacalhau."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### 6. Pin Files Via the IPFS CLI\n",
                "\n",
                "See the web3.storage documentation for [instructions on how to pin files via the IPFS CLI](https://web3.storage/docs/how-tos/pinning-services-api/#using-the-ipfs-cli).\n",
                "\n",
                "#### 7. Pin A File from a URL Via Curl\n",
                "\n",
                "You can use curl to download a file then re-upload to web3.storage. For example:\n",
                "\n",
                "```bash\n",
                "export TOKEN=YOUR_API_KEY\n",
                "curl -o train-images-idx3-ubyte.gz http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
                "curl -X POST https://api.web3.storage/upload -H \"Authorization: Bearer ${TOKEN}\" -H \"X-NAME: train-images-idx3-ubyte.gz\" -d @train-images-idx3-ubyte.gz\n",
                "```\n",
                "\n",
                "Which results in something like:\n",
                "\n",
                "```\n",
                "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
                "                                 Dload  Upload   Total   Spent    Left  Speed\n",
                "100 9680k  100 9680k    0     0  6281k      0  0:00:01  0:00:01 --:--:-- 6318k\n",
                "{\"cid\":\"bafybeiereqxn546lkskldoybaa4xe7wk5fricm33nor4oofrxphlaywwd4\",\"carCid\":\"bagbaieran5ufs752r5vdforovbnjc2gur7kzrsanr3avphsyp7hd6fms7pia\"}%  \n",
                "```\n",
                "\n",
                "#### 8. Pin A File from a URL Via Node.JS\n",
                "\n",
                "You can combine the node.js example above with a `wget` to then upload it to web3.storage.\n",
                "\n",
                "```bash\n",
                "docker run --rm --env TOKEN=$TOKEN -v $PWD/nodejs:/nodejs node:18-alpine ash -c 'cd /nodejs && wget http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz && npm install && node put-files.js --token=$TOKEN train-images-idx3-ubyte.gz'\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.10.2 64-bit ('3.10.2')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.2"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "81ff6dc1b26ee3af5189d22d58a3b36fca4ebbdded7f6a7b0d8f8cebee16ffbc"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
