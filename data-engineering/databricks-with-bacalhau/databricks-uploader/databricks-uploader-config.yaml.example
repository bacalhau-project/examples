# databricks-uploader-config.yaml.example
# Configuration for the SQLite to Databricks Uploader
# 
# IMPORTANT: All fields in this file are REQUIRED unless marked as optional.
# The uploader will exit with an error if any required field is missing.

# === REQUIRED FIELDS ===

# --- Processing Mode Configuration ---
# Determines which table your data will be routed to and what processing is applied.
# Must be one of: RAW, SCHEMATIZED, SANITIZED, AGGREGATED, EMERGENCY (case-insensitive)
# Tables will be named: {databricks_table}_{suffix} where suffix is:
#   - RAW: 0_raw
#   - SCHEMATIZED: 1_schematized  
#   - SANITIZED: 2_sanitized
#   - AGGREGATED: 3_aggregated
#   - EMERGENCY: 4_emergency
processing_mode: "SCHEMATIZED"

# --- SQLite Source Configuration ---
# Path to your SQLite database file. REQUIRED.
# Example: "/path/to/your/sensor_data.db" or "data/sensor_data.db"
sqlite: "path/to/your/sensor_data.db"

# --- Databricks Target Configuration ---
# Databricks Workspace URL. REQUIRED.
# Example: "dbc-12345678-abcd.cloud.databricks.com" (without https://)
databricks_host: "your-databricks-workspace.cloud.databricks.com"

# Databricks HTTP Path. REQUIRED.
# This is the HTTP path for the Databricks SQL warehouse or cluster.
# Example: "/sql/1.0/warehouses/123456789012345b"
databricks_http_path: "/sql/1.0/warehouses/YOUR_WAREHOUSE_ID"

# Target database name in Databricks. REQUIRED.
# Example: "sensor_data", "bronze_layer", "main"
databricks_database: "your_database_name"

# Base table name in Databricks. REQUIRED.
# The actual table name will be: {databricks_table}_{processing_mode_suffix}
# Example: "readings" will become "readings_1_schematized" for SCHEMATIZED mode
databricks_table: "readings"

# --- Uploader Operational Parameters ---
# Directory path where the uploader will store its state file. REQUIRED.
# This file keeps track of the last successfully uploaded timestamp.
# Example: "./state", "/var/lib/uploader/state"
state_dir: "./state"

# Time in seconds between consecutive upload cycles. REQUIRED.
# This determines how frequently the script checks for new data.
# Example: 60 (for 1 minute), 300 (for 5 minutes), 3600 (for 1 hour)
interval: 300

# Maximum number of records to upload in a single batch. REQUIRED.
# Larger batches are more efficient but use more memory.
# Recommended: 500-1000 for stable connections, 100-500 for unreliable networks
max_batch_size: 500

# === OPTIONAL FIELDS ===

# Databricks Personal Access Token (PAT). 
# CRITICAL: Use environment variable DATABRICKS_TOKEN instead for security!
# Only set here for local testing. NEVER commit tokens to version control.
# databricks_token: "dapi..." # USE ENV VAR INSTEAD!

# Name of the table within your SQLite database to read from.
# If omitted, the script will auto-detect a suitable table.
sqlite_table_name: "sensor_logs"

# Name of the timestamp column for incremental loading.
# If omitted, the script will auto-detect a timestamp column.
timestamp_col: "timestamp"

# Fuzz factor for interval randomization (0-1, default: 0.1 = Â±10%)
# Helps prevent thundering herd problems with multiple uploaders
fuzz_factor: 0.1

# --- Processing Mode Specific Configurations ---
# These are optional and only apply to specific processing modes

# GPS Fuzzing Configuration (applies to SCHEMATIZED mode)
gps_fuzzing:
  enabled: true
  radius_meters: 100
  base_lat: 37.7749
  base_lon: -122.4194

# Emergency Detection Configuration (applies to EMERGENCY mode)
emergency_config:
  trigger_keywords: 
    - "EMERGENCY"
    - "ALERT"
    - "CRITICAL"
    - "FAILURE"
  severity_thresholds:
    temperature: 35
    pressure: 1030
    humidity: 90

# Aggregation Configuration (applies to AGGREGATED mode)
aggregate_config:
  window_minutes: 5
  group_by: ["sensor_id"]
  aggregations:
    temperature: ["mean", "max", "min"]
    humidity: ["mean"]
    pressure: ["mean"]

# === CONFIGURATION NOTES ===
# 1. The configuration file is monitored for changes every 2 seconds
# 2. You can change processing_mode while the uploader is running
# 3. Changes to database connection settings require a restart
# 4. Use environment variables for sensitive data:
#    - DATABRICKS_TOKEN (highest priority)
#    - DATABRICKS_HOST
#    - DATABRICKS_HTTP_PATH
#    - DATABRICKS_DATABASE
#    - DATABRICKS_TABLE
#    - PROCESSING_MODE
# 5. Command-line arguments override both env vars and config file