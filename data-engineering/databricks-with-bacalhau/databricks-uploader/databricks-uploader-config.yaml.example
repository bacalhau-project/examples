# databricks-uploader-config.yaml.example
# Configuration for the SQLite to Databricks Uploader
# 
# This uploader routes data to different tables based on processing scenario:
# - Raw data (no processing) -> raw_<table_name>
# - Filtered data -> filtered_<table_name>
# - Sanitized/secure data -> secure_<table_name>
# - Aggregated data -> aggregated_<table_name>

# --- SQLite Source Configuration ---
# Path to your SQLite database file.
# Example: "/path/to/your/sensor_data.db" or "data/sensor_data.db"
sqlite: "path/to/your/sensor_data.db"

# Optional: Name of the table within your SQLite database to read from.
# If omitted, the script will attempt to automatically detect a suitable table.
# Example: "raw_sensor_logs"
sqlite_table: "your_sqlite_table_name"

# Optional: Name of the timestamp column in your SQLite table used for incremental data loading.
# This column should ideally be of a datetime or timestamp type.
# If omitted, the script will attempt to autodetect a column named "timestamp" or one with "date" in its type.
# Example: "reading_timestamp"
timestamp_col: "your_sqlite_timestamp_column"


# --- Databricks Target Configuration ---
# Databricks Workspace URL. REQUIRED.
# This should be the URL of your Databricks workspace.
# It's highly recommended to set this as an environment variable (DATABRICKS_HOST) for security and flexibility.
# Example: "https://your-workspace-id.cloud.databricks.com"
databricks_host: "https://your-databricks-workspace.cloud.databricks.com" # Or set DATABRICKS_HOST env var

# Databricks HTTP Path. REQUIRED.
# This is the HTTP path for the Databricks SQL warehouse or cluster.
# It's highly recommended to set this as an environment variable (DATABRICKS_HTTP_PATH) for security and flexibility.
# Example: "/sql/1.0/warehouses/123456789012345b"
databricks_http_path: "YOUR_DATABRICKS_HTTP_PATH_HERE" # Or set DATABRICKS_HTTP_PATH env var

# Databricks Personal Access Token (PAT). REQUIRED.
# This token is used to authenticate with your Databricks workspace.
# It's CRITICAL to set this as an environment variable (DATABRICKS_TOKEN) for security.
# Do NOT hardcode sensitive tokens in configuration files in production.
# Example: "dapiXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX" (This is a placeholder, use your actual token)
databricks_token: "YOUR_DATABRICKS_PAT_TOKEN_HERE" # Or set DATABRICKS_TOKEN env var (HIGHLY RECOMMENDED)

# Target database name in Databricks. REQUIRED.
# Example: "sensor_data", "bronze_layer", "main.sensordata"
databricks_database: "your_database_name" # Or set DATABRICKS_DATABASE env var

# Base table name in Databricks. REQUIRED.
# This will be prefixed based on processing scenario:
# - Raw data (no processing): raw_<table_name>
# - Filtered data: filtered_<table_name>
# - Sanitized/secure data: secure_<table_name>
# - Aggregated data: aggregated_<table_name>
# Example: "sensor_readings" will become "raw_sensor_readings", "filtered_sensor_readings", etc.
databricks_table: "sensor_readings" # Or set DATABRICKS_TABLE env var


# --- Data Processing Configuration ---
# These settings determine which table your data will be routed to.
# Only enable one processing type at a time for clear data separation.

# Enable data sanitization (routes to secure_* tables)
enable_sanitize: false
sanitize_config:
  # Example: Remove null values from specific columns
  # remove_nulls: ["temperature", "humidity"]
  # Example: Replace specific values
  # replace_values:
  #   status: {"error": "unknown", "": "unknown"}

# Enable data filtering (routes to filtered_* tables)  
enable_filter: false
filter_config:
  # Example: Filter by temperature range
  # temperature:
  #   ">": -50
  #   "<": 150
  # Example: Filter by status
  # device_status: "active"

# Enable data aggregation (routes to aggregated_* tables)
enable_aggregate: false  
aggregate_config:
  # Example: Group by device and hour
  # group_by: ["device_id", "hour"]
  # aggregations:
  #   temperature: "avg"
  #   humidity: "max"
  #   readings: "count"

# --- Uploader Operational Parameters ---
# Directory path where the uploader will store its state file (last_upload.json).
# This file keeps track of the last successfully uploaded timestamp to enable incremental loading.
# Ensure the script has write permissions to this directory.
# Example: "/var/run/sqlite_uploader_state" or "./uploader_state"
state_dir: "/path/to/your/state_directory" # Directory to store the last upload timestamp

# Time in seconds between consecutive upload cycles.
# This determines how frequently the script checks for new data in the SQLite database.
# This value is ignored if the --once command-line flag is used.
# Default is 300 seconds (5 minutes) if not specified.
# Example: 60 (for 1 minute), 3600 (for 1 hour)
interval: 300

# To run the script once and then exit (instead of continuous monitoring),
# use the --once command-line flag when running the script.
# Example: python sqlite_to_delta_uploader.py --config uploader-config.yaml --once
# once: false # This line is commented out as --once is preferred via CLI.
