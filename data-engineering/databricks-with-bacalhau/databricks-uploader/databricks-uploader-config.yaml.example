# databricks-uploader-config.yaml.example
# Configuration for the SQLite to Databricks Uploader
# 
# IMPORTANT: All fields in this file are REQUIRED unless marked as optional.
# The uploader will exit with an error if any required field is missing.

# === REQUIRED FIELDS ===

# --- Pipeline Type Configuration ---
# IMPORTANT: Pipeline type is now managed in the SQLite database for atomic updates.
# Pipeline type determines which table your data will be routed to:
#   - raw: {databricks_table}_0_raw
#   - schematized: {databricks_table}_1_schematized  
#   - filtered: {databricks_table}_2_filtered (replaces sanitized)
#   - emergency: {databricks_table}_4_emergency (replaces aggregated)
#
# To manage pipeline types, use the pipeline_manager.py tool:
#   View current pipeline:
#     uv run -s pipeline_manager.py --db /path/to/sensor_data.db get
#   
#   Change pipeline type:
#     uv run -s pipeline_manager.py --db /path/to/sensor_data.db set filtered --by "your_name"
#   
#   View execution history:
#     uv run -s pipeline_manager.py --db /path/to/sensor_data.db history --limit 20

# --- SQLite Source Configuration ---
# Path to your SQLite database file.
# Example: "/path/to/your/sensor_data.db" or "data/sensor_data.db"
sqlite: "path/to/your/sensor_data.db"

# Optional: Name of the table within your SQLite database to read from.
# If omitted, the script will attempt to automatically detect a suitable table.
# Example: "raw_sensor_logs"
sqlite_table: "your_sqlite_table_name"

# Optional: Name of the timestamp column in your SQLite table used for incremental data loading.
# This column should ideally be of a datetime or timestamp type.
# If omitted, the script will attempt to autodetect a column named "timestamp" or one with "date" in its type.
# Example: "reading_timestamp"
timestamp_col: "your_sqlite_timestamp_column"


# --- Databricks Target Configuration ---
# Databricks Workspace URL. REQUIRED.
# This should be the URL of your Databricks workspace.
# It's highly recommended to set this as an environment variable (DATABRICKS_HOST) for security and flexibility.
# Example: "https://your-workspace-id.cloud.databricks.com"
databricks_host: "https://your-databricks-workspace.cloud.databricks.com" # Or set DATABRICKS_HOST env var

# Databricks HTTP Path. REQUIRED.
# This is the HTTP path for the Databricks SQL warehouse or cluster.
# It's highly recommended to set this as an environment variable (DATABRICKS_HTTP_PATH) for security and flexibility.
# Example: "/sql/1.0/warehouses/123456789012345b"
databricks_http_path: "YOUR_DATABRICKS_HTTP_PATH_HERE" # Or set DATABRICKS_HTTP_PATH env var

# Databricks Personal Access Token (PAT). REQUIRED.
# This token is used to authenticate with your Databricks workspace.
# It's CRITICAL to set this as an environment variable (DATABRICKS_TOKEN) for security.
# Do NOT hardcode sensitive tokens in configuration files in production.
# Example: "dapiXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX" (This is a placeholder, use your actual token)
databricks_token: "YOUR_DATABRICKS_PAT_TOKEN_HERE" # Or set DATABRICKS_TOKEN env var (HIGHLY RECOMMENDED)

# Target database name in Databricks. REQUIRED.
# Example: "sensor_data", "bronze_layer", "main.sensordata"
databricks_database: "your_database_name" # Or set DATABRICKS_DATABASE env var

# Base table name in Databricks. REQUIRED.
# This will be prefixed based on processing scenario:
# - Raw data (no processing): raw_<table_name>
# - Filtered data: filtered_<table_name>
# - Sanitized/secure data: secure_<table_name>
# - Aggregated data: aggregated_<table_name>
# Example: "sensor_readings" will become "raw_sensor_readings", "filtered_sensor_readings", etc.
databricks_table: "sensor_readings" # Or set DATABRICKS_TABLE env var


# --- Data Processing Configuration ---
# These settings determine which table your data will be routed to.
# Only enable one processing type at a time for clear data separation.

# Enable data sanitization (routes to secure_* tables)
enable_sanitize: false
sanitize_config:
  # Example: Remove null values from specific columns
  # remove_nulls: ["temperature", "humidity"]
  # Example: Replace specific values
  # replace_values:
  #   status: {"error": "unknown", "": "unknown"}

# Enable data filtering (routes to filtered_* tables)  
enable_filter: false
filter_config:
  # Example: Filter by temperature range
  # temperature:
  #   ">": -50
  #   "<": 150
  # Example: Filter by status
  # device_status: "active"

# Enable data aggregation (routes to aggregated_* tables)
enable_aggregate: false  
aggregate_config:
  # Example: Group by device and hour
  # group_by: ["device_id", "hour"]
  # aggregations:
  #   temperature: "avg"
  #   humidity: "max"
  #   readings: "count"

# --- Uploader Operational Parameters ---
# Directory path where the uploader will store its state file (last_upload.json).
# This file keeps track of the last successfully uploaded timestamp to enable incremental loading.
# Ensure the script has write permissions to this directory.
# Example: "/var/run/sqlite_uploader_state" or "./uploader_state"
state_dir: "/path/to/your/state_directory" # Directory to store the last upload timestamp

# Time in seconds between consecutive upload cycles. REQUIRED.
# This determines how frequently the script checks for new data.
# Example: 60 (for 1 minute), 300 (for 5 minutes), 3600 (for 1 hour)
interval: 30

# Maximum number of records to upload in a single batch. REQUIRED.
# Larger batches are more efficient but use more memory.
# Recommended: 500-1000 for stable connections, 100-500 for unreliable networks
max_batch_size: 500

# === OPTIONAL FIELDS ===

# Databricks Personal Access Token (PAT). 
# CRITICAL: Use environment variable DATABRICKS_TOKEN instead for security!
# Only set here for local testing. NEVER commit tokens to version control.
# databricks_token: "dapi..." # USE ENV VAR INSTEAD!

# Name of the table within your SQLite database to read from.
# If omitted, the script will auto-detect a suitable table.
sqlite_table_name: "sensor_logs"

# Name of the timestamp column for incremental loading.
# If omitted, the script will auto-detect a timestamp column.
timestamp_col: "timestamp"

# Fuzz factor for interval randomization (0-1, default: 0.1 = Â±10%)
# Helps prevent thundering herd problems with multiple uploaders
fuzz_factor: 0.1

# --- Processing Mode Specific Configurations ---
# These are optional and only apply to specific processing modes

# GPS Fuzzing Configuration (applies to schematized pipeline)
gps_fuzzing:
  enabled: true
  radius_meters: 100
  base_lat: 37.7749
  base_lon: -122.4194

# Emergency Detection Configuration (applies to emergency pipeline)
emergency_config:
  trigger_keywords: 
    - "EMERGENCY"
    - "ALERT"
    - "CRITICAL"
    - "FAILURE"
  severity_thresholds:
    temperature: 35
    pressure: 1030
    humidity: 90

# Aggregation Configuration (applies to emergency pipeline)
aggregate_config:
  window_minutes: 5
  group_by: ["sensor_id"]
  aggregations:
    temperature: ["mean", "max", "min"]
    humidity: ["mean"]
    pressure: ["mean"]

# === CONFIGURATION NOTES ===
# 1. The configuration file is monitored for changes every 2 seconds
# 2. Pipeline type changes must be done through pipeline_manager.py (not config file)
# 3. Changes to database connection settings require a restart
# 4. Use environment variables for sensitive data:
#    - DATABRICKS_TOKEN (highest priority)
#    - DATABRICKS_HOST
#    - DATABRICKS_HTTP_PATH
#    - DATABRICKS_DATABASE
#    - DATABRICKS_TABLE
# 5. Command-line arguments override both env vars and config file
# 6. Old configs with processing_mode or enable_* flags will be automatically migrated