# Environmental Sensor Data Pipeline with Databricks and Bacalhau

A production-ready data pipeline for processing environmental sensor data (temperature, humidity, pressure, vibration, voltage) with real-time anomaly detection, multi-stage transformation, and Databricks Unity Catalog integration.

## üöÄ Quick Start

**For complete setup and demo instructions, see: [`MASTER_SETUP_AND_DEMO.md`](MASTER_SETUP_AND_DEMO.md)**

This is the single, authoritative guide that will take you from zero to a fully working demo.

## üìÅ Project Structure

```
.
‚îú‚îÄ‚îÄ MASTER_SETUP_AND_DEMO.md    # ‚≠ê START HERE - Complete setup guide
‚îú‚îÄ‚îÄ .env.example                 # Environment configuration template
‚îú‚îÄ‚îÄ databricks-notebooks/        # Databricks AutoLoader notebooks
‚îÇ   ‚îî‚îÄ‚îÄ setup-and-run-autoloader.py  # Main AutoLoader notebook
‚îú‚îÄ‚îÄ databricks-uploader/         # Data validation and upload service
‚îÇ   ‚îú‚îÄ‚îÄ sqlite_to_databricks_uploader.py
‚îÇ   ‚îú‚îÄ‚îÄ environmental_sensor_models.py  # Pydantic models for sensor data
‚îÇ   ‚îú‚îÄ‚îÄ environmental_transformer.py    # Data transformation logic
‚îÇ   ‚îî‚îÄ‚îÄ pipeline_manager.py
‚îú‚îÄ‚îÄ scripts/                     # Automation and utility scripts
‚îÇ   ‚îú‚îÄ‚îÄ validate-env.sh         # Validate environment configuration
‚îÇ   ‚îú‚îÄ‚îÄ create-all-buckets.sh   # Create S3 buckets
‚îÇ   ‚îú‚îÄ‚îÄ seed-buckets-for-autoloader.py  # Seed buckets with sample data
‚îÇ   ‚îú‚îÄ‚îÄ fix-external-locations-individual.py  # Fix external location URLs
‚îÇ   ‚îú‚îÄ‚îÄ turbo-delete-buckets.py # Fast bucket deletion
‚îÇ   ‚îú‚îÄ‚îÄ clean-all-data.sh       # Clean bucket contents
‚îÇ   ‚îú‚îÄ‚îÄ start-environmental-sensor.sh  # Start environmental sensor
‚îÇ   ‚îî‚îÄ‚îÄ run-anomaly-demo.sh     # Run complete demo
‚îú‚îÄ‚îÄ docs/                        # Additional documentation
‚îÇ   ‚îú‚îÄ‚îÄ DEVELOPMENT_RULES.md
‚îÇ   ‚îú‚îÄ‚îÄ ENVIRONMENT_SETUP.md
‚îÇ   ‚îî‚îÄ‚îÄ QUICK_START_CHECKLIST.md
‚îî‚îÄ‚îÄ jobs/                        # Bacalhau job specifications
    ‚îî‚îÄ‚îÄ databricks-uploader-job.yaml
```

## üéØ Key Features

- **Multi-Stage Pipeline**: Raw ‚Üí Validated ‚Üí Anomalies ‚Üí Schematized ‚Üí Aggregated data flow
- **Anomaly Detection**: Physics-based validation for wind turbine data
- **Real-Time Processing**: Streaming ingestion with Databricks AutoLoader
- **Schema Evolution**: Automatic schema inference and validation
- **Unity Catalog**: Enterprise governance and data management
- **Containerized Services**: Docker-based sensors and uploaders

## üîß Architecture

```mermaid
graph LR
    A[Sensor] --> B[SQLite]
    B --> C[Uploader]
    C --> D{Validator}
    D -->|Valid| E[Validated S3]
    D -->|Invalid| F[Anomalies S3]
    E --> G[AutoLoader]
    F --> G
    G --> H[Databricks]
    H --> I[Unity Catalog]
```

## üìä Pipeline Stages

1. **Raw Data**: All sensor readings as received
2. **Validated Data**: Readings that pass physics validation
3. **Anomalies**: Readings that violate physics rules
4. **Schematized Data**: Structured with enforced schema
5. **Aggregated Data**: Analytics-ready summaries

## üö¶ Anomaly Detection Rules

The system detects anomalies in environmental sensor data:
- **Temperature anomalies**: Values outside -20¬∞C to 60¬∞C range
- **Humidity anomalies**: Values outside 5% to 95% range  
- **Pressure anomalies**: Values outside 950-1050 hPa range
- **Vibration anomalies**: Values exceeding 10 mm/s¬≤
- **Voltage anomalies**: Values outside 20-25V range
- **Sensor-flagged anomalies**: Records with anomaly_flag = 1 from sensor

## Prerequisites

- Python 3.11+
- Docker
- AWS Account with S3 access
- Databricks Workspace with Unity Catalog enabled
- `uv` package manager (`pip install uv`)
- Bacalhau CLI v1.5.0+

## üèÉ Complete Setup Process

### Phase 1: Environment Setup

1. **Clone and Configure**:
```bash
# Clone the repository
git clone <repo-url>
cd databricks-with-bacalhau

# Copy and configure environment
cp .env.example .env
# Edit .env with your credentials
```

2. **Validate Configuration**:
```bash
./scripts/validate-env.sh
```

### Phase 2: AWS Infrastructure

1. **Create S3 Buckets**:
```bash
./scripts/create-all-buckets.sh
```

This creates all required buckets:
- `expanso-raw-data-{region}`
- `expanso-validated-data-{region}`
- `expanso-anomalies-data-{region}`
- `expanso-schematized-data-{region}`
- `expanso-aggregated-data-{region}`
- `expanso-checkpoints-{region}`
- `expanso-metadata-{region}`

2. **Setup IAM Role**:
```bash
./scripts/create-databricks-iam-role.sh
./scripts/update-iam-role-for-new-buckets.sh
```

### Phase 3: Databricks Setup

1. **Setup Unity Catalog**:
```bash
cd scripts
uv run -s setup-unity-catalog-storage.py
```

2. **Fix External Locations** (Critical!):
```bash
# External locations may have wrong URLs - fix them
uv run -s fix-external-locations-individual.py
```

3. **Seed Buckets with Sample Data**:
```bash
# AutoLoader needs sample files to infer schemas
uv run -s seed-buckets-for-autoloader.py
```

4. **Upload and Run AutoLoader Notebook**:
```bash
uv run -s upload-and-run-notebook.py \
  --notebook ../databricks-notebooks/setup-and-run-autoloader.py
```

### Phase 4: Run the Demo

1. **Start Environmental Sensor**:
```bash
# Normal sensor (no anomalies)
./scripts/start-environmental-sensor.sh 300

# With anomalies (25% probability)
./scripts/start-environmental-sensor.sh 300 --with-anomalies
```

2. **Monitor Processing**:
   - Open Databricks workspace
   - Navigate to the uploaded notebook
   - Watch as data flows through all 5 pipeline stages
   - View anomalies being detected and routed

3. **Query Results**:
```sql
-- View ingested data
SELECT * FROM expanso_catalog.sensor_data.sensor_readings_ingestion;

-- View detected anomalies
SELECT * FROM expanso_catalog.sensor_data.sensor_readings_anomalies;

-- View aggregated metrics
SELECT * FROM expanso_catalog.sensor_data.sensor_readings_aggregated;
```

## üîç Troubleshooting

### Common Issues and Solutions

1. **AutoLoader Schema Inference Error**:
   - **Cause**: Empty buckets
   - **Solution**: Run `scripts/seed-buckets-for-autoloader.py`

2. **UNAUTHORIZED_ACCESS Error**:
   - **Cause**: External locations pointing to wrong bucket URLs
   - **Solution**: Run `scripts/fix-external-locations-individual.py`

3. **Permission Denied on External Locations**:
   - **Cause**: IAM role not updated for new buckets
   - **Solution**: Run `scripts/update-iam-role-for-new-buckets.sh`

4. **No Data Flowing**:
   - **Cause**: Checkpoints preventing reprocessing
   - **Solution**: Clean checkpoints with `scripts/clean-all-data.sh`

### Verification Scripts

```bash
# Check bucket structure
./scripts/check-bucket-structure.sh

# Verify Databricks setup
cd scripts && uv run -s verify-databricks-setup.py

# List external locations
uv run --with databricks-sdk --with python-dotenv python3 -c "
from databricks.sdk import WorkspaceClient
from dotenv import load_dotenv
import os
from pathlib import Path

load_dotenv(Path('.').parent / '.env')
w = WorkspaceClient(host=os.getenv('DATABRICKS_HOST'), token=os.getenv('DATABRICKS_TOKEN'))

print('External Locations:')
for loc in w.external_locations.list():
    if 'expanso' in loc.name.lower():
        print(f'  {loc.name}: {loc.url}')
"
```

## üìà Monitoring

### Key Metrics to Track

- **Ingestion Rate**: Files/second being processed
- **Anomaly Rate**: Percentage of readings flagged
- **Processing Latency**: Time from sensor to Unity Catalog
- **Schema Evolution**: New columns being added

### Dashboard Queries

```sql
-- Anomaly detection rate
SELECT 
    DATE(processing_timestamp) as date,
    COUNT(*) as anomaly_count,
    AVG(wind_speed) as avg_wind_speed,
    AVG(power_output) as avg_power
FROM expanso_catalog.sensor_data.sensor_readings_anomalies
GROUP BY DATE(processing_timestamp)
ORDER BY date DESC;

-- Pipeline throughput
SELECT 
    stage,
    COUNT(*) as record_count,
    MAX(processing_timestamp) as last_update
FROM (
    SELECT 'ingestion' as stage, processing_timestamp 
    FROM expanso_catalog.sensor_data.sensor_readings_ingestion
    UNION ALL
    SELECT 'validated' as stage, processing_timestamp 
    FROM expanso_catalog.sensor_data.sensor_readings_validated
    UNION ALL
    SELECT 'anomalies' as stage, processing_timestamp 
    FROM expanso_catalog.sensor_data.sensor_readings_anomalies
)
GROUP BY stage;
```

## üßπ Cleanup

To completely clean up all resources:

```bash
# Delete all S3 buckets and contents
./scripts/turbo-delete-buckets.py

# Clean Unity Catalog objects
cd scripts && uv run -s cleanup-unity-catalog.py

# Remove local artifacts
rm -rf .flox/run .env *.db *.log
```

## üìö Additional Resources

- [Development Rules](docs/DEVELOPMENT_RULES.md) - Coding standards and best practices
- [Environment Setup Guide](docs/ENVIRONMENT_SETUP.md) - Detailed environment configuration
- [Quick Start Checklist](docs/QUICK_START_CHECKLIST.md) - Step-by-step verification
- [Master Setup Guide](MASTER_SETUP_AND_DEMO.md) - Complete walkthrough

## ü§ù Contributing

1. Follow the coding standards in `docs/DEVELOPMENT_RULES.md`
2. Always use `uv` for Python scripts
3. Test with both normal and anomaly data
4. Update documentation for any new features

## üìù License

MIT License - See LICENSE file for details

## üÜò Support

For issues or questions:
1. Check the [Master Setup Guide](MASTER_SETUP_AND_DEMO.md)
2. Review troubleshooting section above
3. Open an issue with:
   - Environment details (`.env` without secrets)
   - Error messages and logs
   - Steps to reproduce

## üéâ Success Indicators

You know the pipeline is working when:
- ‚úÖ All 5 S3 buckets have data flowing
- ‚úÖ Unity Catalog tables are populated
- ‚úÖ Anomalies are being detected and routed
- ‚úÖ AutoLoader is processing files continuously
- ‚úÖ No errors in Databricks notebook execution