# uploader-config.yaml.example
# Configuration for the continuous SQLiteâ†’Databricks uploader
#
# `sqlite_path`: Path inside the container to the mounted SQLite database file.
# `state_dir`: Path inside the container to persist last-upload state file.
# `interval`: Seconds between upload cycles.
# `sqlite_table_name`: (Optional) Name of the SQLite table to read from. If not provided, attempts to auto-detect.
# `timestamp_col`: (Optional) Name of the timestamp column in the SQLite table. If not provided, attempts to auto-detect.

# --- Databricks Connection Parameters ---
# It is STRONGLY recommended to use environment variables for sensitive data like tokens.
databricks_host: "your-workspace.azuredatabricks.net" # Example: "adb-xxxx.azuredatabricks.net"
databricks_http_path: "/sql/1.0/warehouses/your_sql_warehouse_http_path" # Example: "/sql/1.0/warehouses/12345abcde"
# databricks_token: "dapixxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx" # Example: "dapi123..."
# IMPORTANT: Avoid hardcoding tokens. Prefer DATABRICKS_TOKEN environment variable.
databricks_database: "your_database" # Example: "default" or "bronze_layer"
databricks_table: "your_table"     # Example: "sensor_readings"

# --- SQLite Source Parameters ---
sqlite_path: "/data/sensor.db"
sqlite_table_name: "sensor_data" # Example: "my_sqlite_table_name", uncomment to use
# timestamp_col: "event_timestamp" # Example: "created_at", uncomment to use

# --- Uploader Control Parameters ---
state_dir: "/state"
interval: 300

# Optional overrides via environment variables:
#   SQLITE_PATH              override `sqlite_path`
#   SQLITE_TABLE_NAME        override `sqlite_table_name` # (previously TABLE_NAME)
#   TIMESTAMP_COL            override `timestamp_col`
#   STATE_DIR                override `state_dir`
#   UPLOAD_INTERVAL          override `interval`
#
#   DATABRICKS_HOST          override `databricks_host`
#   DATABRICKS_HTTP_PATH     override `databricks_http_path`
#   DATABRICKS_TOKEN         override `databricks_token` (Recommended for security)
#   DATABRICKS_DATABASE      override `databricks_database`
#   DATABRICKS_TABLE         override `databricks_table`

# --- Local Data Processing ---
# Enable or disable local data processing steps before uploading.
# Configurations for these steps can be provided as YAML structures (preferred) or JSON strings (for CLI/env).

# Enable data sanitization (true/false)
enable_sanitize: false
# Configuration for sanitization.
# Example: rules to apply to specific columns.
sanitize_config:
  # column_to_clean: "pattern_to_remove" # Example: remove a specific pattern
  # another_column: { "replace_value": "X", "with_value": "Y" } # Example: replace specific values
  default_string_clean: "trim" # Example: trim whitespace from all strings, if a global string op is supported
  # Or leave empty if no sanitization is active by default:
  # sanitize_config: {}

# Enable data filtering (true/false)
enable_filter: false
# Configuration for filtering.
# Example: conditions to keep rows.
filter_config:
  # numeric_column: { greater_than: 100, less_than_or_equal_to: 200 } # Example: range filter
  # string_column_equals: "specific_value" # Example: exact match filter
  # remove_if_column_is_null: ["critical_column1", "critical_column2"] # Example: remove rows with nulls
  # Example: keep rows where sensor_x has a value greater than 10
  # minimum_value_for_sensor_x: 10
  # Or leave empty if no filtering is active by default:
  filter_config: {}


# Enable data aggregation (true/false)
enable_aggregate: false
# Configuration for aggregation.
# Example: group by columns and define aggregate functions.
aggregate_config:
  # Example: Aggregate sensor readings over a category
  # group_by_columns: ["category"]
  # aggregate_functions:
  #   value: "sum" # Sum of 'value' column
  #   records: "size" # Count of records in each group
  # More complex example:
  # group_by_columns: ["device_id", "time_window_5min"] # Group by device and 5-min time window
  # aggregate_functions:
  #   temperature: "avg" # Average temperature
  #   humidity: "max"  # Max humidity
  #   reading_count: "count" # Number of readings
  # Or leave empty if no aggregation is active by default:
  aggregate_config: {}

# Optional overrides for processing steps via environment variables:
#   ENABLE_SANITIZE   override `enable_sanitize` (e.g., "true" or "false")
#   SANITIZE_CONFIG   override `sanitize_config` (JSON string for env var, YAML structure for file)
#   ENABLE_FILTER     override `enable_filter`
#   FILTER_CONFIG     override `filter_config` (JSON string for env var, YAML structure for file)
#   ENABLE_AGGREGATE  override `enable_aggregate`
#   AGGREGATE_CONFIG  override `aggregate_config` (JSON string for env var, YAML structure for file)