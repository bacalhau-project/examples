# uploader-config.yaml.example
# Configuration for the continuous SQLiteâ†’Delta Lake uploader
#
# `sqlite`: Path inside the container to the mounted SQLite database file.
# `storage_uri`: URI of the Delta Lake table (e.g., on S3 or ADLS or local file://) to append to.
# `state_dir`: Path inside the container to persist last-upload state file.
# `interval`: Seconds between upload cycles.
# `table`: (Optional) Name of the SQLite table to read from. If not provided, attempts to auto-detect.
# `timestamp_col`: (Optional) Name of the timestamp column in the SQLite table. If not provided, attempts to auto-detect.

sqlite: "/data/sensor.db"
storage_uri: "s3://<your-bucket>/delta/bacalhau_results/sensor_readings" # Or e.g., file:///data/delta/sensor_readings
state_dir: "/state"
interval: 300
# table: "sensor_data" # Example: "my_table_name"
# timestamp_col: "event_timestamp" # Example: "created_at"

# Optional overrides via environment variables:
#   SQLITE_PATH       override `sqlite`
#   STORAGE_URI       override `storage_uri` (this replaces the old TABLE_PATH)
#   TABLE_NAME        override `table`
#   TIMESTAMP_COL     override `timestamp_col`
#   STATE_DIR         override `state_dir`
#   UPLOAD_INTERVAL   override `interval`

# --- Local Data Processing ---
# Enable or disable local data processing steps before uploading.
# Configurations for these steps can be provided as YAML structures or JSON strings.

# Enable data sanitization (true/false)
enable_sanitize: false
# Configuration for sanitization.
# Example: rules to apply to specific columns.
sanitize_config:
  # column_to_clean: "pattern_to_remove" # Example: remove a specific pattern
  # another_column: { "replace_value": "X", "with_value": "Y" } # Example: replace specific values
  default_string_clean: "trim" # Example: trim whitespace from all strings, if a global string op is supported
  # Or leave empty if no sanitization is active by default:
  # sanitize_config: {}

# Enable data filtering (true/false)
enable_filter: false
# Configuration for filtering.
# Example: conditions to keep rows.
filter_config:
  # numeric_column: { greater_than: 100, less_than_or_equal_to: 200 } # Example: range filter
  # string_column_equals: "specific_value" # Example: exact match filter
  # remove_if_column_is_null: ["critical_column1", "critical_column2"] # Example: remove rows with nulls
  # Example: keep rows where sensor_x has a value greater than 10
  # minimum_value_for_sensor_x: 10
  # Or leave empty if no filtering is active by default:
  filter_config: {}


# Enable data aggregation (true/false)
enable_aggregate: false
# Configuration for aggregation.
# Example: group by columns and define aggregate functions.
aggregate_config:
  # Example: Aggregate sensor readings over a category
  # group_by_columns: ["category"]
  # aggregate_functions:
  #   value: "sum" # Sum of 'value' column
  #   records: "size" # Count of records in each group
  # More complex example:
  # group_by_columns: ["device_id", "time_window_5min"] # Group by device and 5-min time window
  # aggregate_functions:
  #   temperature: "avg" # Average temperature
  #   humidity: "max"  # Max humidity
  #   reading_count: "count" # Number of readings
  # Or leave empty if no aggregation is active by default:
  aggregate_config: {}

# Optional overrides for processing steps via environment variables:
#   ENABLE_SANITIZE   override `enable_sanitize` (e.g., "true" or "false")
#   SANITIZE_CONFIG   override `sanitize_config` (JSON string OR native YAML if env var parser supports it - typically JSON string)
#   ENABLE_FILTER     override `enable_filter`
#   FILTER_CONFIG     override `filter_config` (JSON string OR native YAML)
#   ENABLE_AGGREGATE  override `enable_aggregate`
#   AGGREGATE_CONFIG  override `aggregate_config` (JSON string OR native YAML)