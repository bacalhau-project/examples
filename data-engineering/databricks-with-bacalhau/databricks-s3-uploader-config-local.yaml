# SQLite to S3 Uploader Configuration - Local Version

# SQLite Configuration
sqlite: "../sample-sensor/data/sensor_data.db" # Path to the sensor database
sqlite_table: "sensor_readings" # The table name in this database (sensor writes here)
timestamp_col: "timestamp" # Changed from timestamp_column

# S3 Configuration
s3_configuration:
  enabled: true
  region: "us-west-2"
  prefix: "expanso"
  # AWS credentials (optional - will use environment variables if not set)
  # access_key_id: "your-access-key"
  # secret_access_key: "your-secret-key"
  # Buckets for each scenario
  buckets:
    ingestion: "expanso-databricks-ingestion-us-west-2"
    validated: "expanso-databricks-validated-us-west-2"
    enriched: "expanso-databricks-enriched-us-west-2"
    aggregated: "expanso-databricks-aggregated-us-west-2"
    anomalies: "expanso-anomalies-us-west-2"

# Processing Configuration
state_dir: "state"
upload_interval: 15 # seconds
max_batch_size: 500
enable_auto_detection: true

# S3 upload settings
s3_batch_size_mb: 50 # Target size for Parquet files
s3_compression: "snappy"