#!/bin/bash
# Template for additional-commands.sh
# COPY THIS FILE TO additional-commands.sh AND ADD YOUR ACTUAL CREDENTIALS
# WARNING: NEVER COMMIT additional-commands.sh TO GIT!

# AWS Credentials for S3 uploads
export AWS_ACCESS_KEY_ID="YOUR-ACTUAL-ACCESS-KEY-HERE"
export AWS_SECRET_ACCESS_KEY="YOUR-ACTUAL-SECRET-KEY-HERE"
export AWS_REGION="us-west-2"

# Databricks credentials (if needed)
export DATABRICKS_HOST="https://your-databricks-workspace.cloud.databricks.com"
export DATABRICKS_TOKEN="your-databricks-token-here"

# Make credentials available to all processes
# shellcheck disable=SC2129
echo "export AWS_ACCESS_KEY_ID='$AWS_ACCESS_KEY_ID'" >> /etc/profile.d/aws-creds.sh
echo "export AWS_SECRET_ACCESS_KEY='$AWS_SECRET_ACCESS_KEY'" >> /etc/profile.d/aws-creds.sh
echo "export AWS_REGION='$AWS_REGION'" >> /etc/profile.d/aws-creds.sh

# Ensure the credentials are available in the bacalhau node directory
mkdir -p /bacalhau_node
cat > /bacalhau_node/aws-credentials.env << EOF
AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
AWS_REGION=$AWS_REGION
DATABRICKS_HOST=$DATABRICKS_HOST
DATABRICKS_TOKEN=$DATABRICKS_TOKEN
EOF

# Set proper permissions
chmod 600 /bacalhau_node/aws-credentials.env
chmod +x /etc/profile.d/aws-creds.sh

echo "âœ… AWS and Databricks credentials configured for Bacalhau node"

# Keep the rest of the existing additional-commands.sh content below...