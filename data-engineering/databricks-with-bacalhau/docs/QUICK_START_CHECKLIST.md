# âš¡ Quick Start Checklist
## Autoloader-Only Pipeline - Step-by-Step Verification

**Copy and execute each command, verifying the expected output before proceeding.**

---

## âœ… Prerequisites Check

### 1. Install uv
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```
**âœ… Expected:** Installation success message

### 2. Verify Directory
```bash
cd /Users/daaronch/code/bacalhau-examples/data-engineering/databricks-with-bacalhau/databricks-uploader
pwd
```
**âœ… Expected:** `/Users/daaronch/code/bacalhau-examples/data-engineering/databricks-with-bacalhau/databricks-uploader`

### 3. Test uv
```bash
uv --version
```
**âœ… Expected:** `uv 0.x.x` (any recent version)

---

## ðŸ”§ Environment Setup

### 4. Set Environment Variables
```bash
# REQUIRED - Replace with your actual values
export DATABRICKS_HOST="your-workspace.databricks.com"
export DATABRICKS_HTTP_PATH="/sql/1.0/endpoints/your-endpoint"
export DATABRICKS_TOKEN="your-databricks-token"
export AWS_REGION="us-east-1"
export DATABRICKS_IAM_ROLE="arn:aws:iam::123456789012:role/databricks-unity-catalog-role"

# OPTIONAL - Uses defaults if not set
export UC_CATALOG="sensor_data_catalog"
export UC_SCHEMA="sensor_pipeline"
```

### 5. Verify Environment
```bash
env | grep -E "(DATABRICKS|AWS|UC_)" | sort
```
**âœ… Expected:** All variables listed with your values

---

## ðŸ—ï¸ Infrastructure Setup

### 6. Setup Infrastructure
```bash
uv run -s autoloader_main.py setup
```
**âœ… Expected:** 
- "âœ… Storage credential 'sensor_pipeline_storage_cred' created"
- "âœ… External location 'sensor_data_raw_location' created" (x5 locations)
- "âœ… UC Volume 'autoloader_checkpoints' created" (x2 volumes)
- "âœ… Autoloader table 'sensor_readings_raw' created" (x4 tables)
- "âœ… Autoloader infrastructure setup completed!"

### 7. Verify Infrastructure
```bash
uv run -s autoloader_main.py status
```
**âœ… Expected:**
- "ðŸ—ï¸ Unity Catalog Infrastructure: Tables: 4"
- "âœ… sensor_readings_raw: 0 records" (x4 tables)
- "âœ… S3 Access: Valid"
- "âœ… Overall Status: healthy"

---

## ðŸ“Š Data Processing

### 8. Create Test Data
```bash
python create_test_sensor_data.py
ls -la sensor_data.db
```
**âœ… Expected:** 
- File creation messages
- `-rw-r--r-- ... sensor_data.db` (file exists)

### 9. Verify Test Data
```bash
sqlite3 sensor_data.db "SELECT COUNT(*) FROM sensor_data;"
```
**âœ… Expected:** `1000` (or another number > 0)

### 10. Process Single Batch
```bash
uv run -s autoloader_main.py process --db-path sensor_data.db
```
**âœ… Expected:**
- "ðŸ“Š Processing data with Autoloader..."
- "ðŸ“Š Processing Results:"
- "Pipeline: schematized"
- "Records: 1000"
- "Uploads: 4" with S3 URIs

### 11. Verify Processing
```bash
uv run -s autoloader_main.py status
```
**âœ… Expected:**
- Tables showing > 0 records:
  - "âœ… sensor_readings_raw: 1000 records"
  - "âœ… sensor_readings_schematized: 850 records" (filtered count)
  - etc.

---

## ðŸ“Š Monitoring

### 12. Start Monitoring (Background)
```bash
uv run -s autoloader_main.py monitor --port 8000 &
sleep 5  # Wait for startup
```
**âœ… Expected:** 
- "ðŸš€ Dashboard starting at http://0.0.0.0:8000"
- "INFO: Uvicorn running on http://0.0.0.0:8000"

### 13. Test API Health
```bash
curl http://localhost:8000/api/health
```
**âœ… Expected:**
```json
{
  "status": "healthy",
  "timestamp": "2025-01-03T...",
  "components": {
    "databricks": {"status": "connected"},
    "s3": {"status": "accessible"}
  }
}
```

### 14. Test Pipeline Status
```bash
curl http://localhost:8000/api/pipelines/status
```
**âœ… Expected:** JSON with pipeline status and record counts

### 15. Stop Monitoring
```bash
pkill -f "autoloader_main.py monitor"
```
**âœ… Expected:** Process terminated

---

## ðŸ§ª Testing

### 16. Run All Tests
```bash
uv run -s autoloader_main.py test --type all
```
**âœ… Expected:**
- "ðŸ§ª Running Raw Pipeline Test... âœ… Raw data processing test passed"
- "ðŸ§ª Running Schematized Pipeline Test... âœ… Schema validation test passed"
- "ðŸ§ª Running Emergency Detection Test... âœ… Anomaly detection test passed"
- "ðŸ§ª Running S3 Access Validation Test... âœ… S3 bucket access test passed"
- "ðŸ§ª Running Data Partitioning Test... âœ… Date partitioning test passed"
- "âœ… All tests completed successfully!"

### 17. Test Individual Components
```bash
uv run -s autoloader_main.py test --type s3-access
```
**âœ… Expected:** "âœ… s3-access test completed successfully!"

```bash
uv run -s autoloader_main.py test --type emergency
```
**âœ… Expected:** "âœ… emergency test completed successfully!"

---

## ðŸ”„ Continuous Processing (Optional)

### 18. Test Continuous Mode (30 seconds)
```bash
timeout 30 uv run -s autoloader_main.py process --continuous --interval 10
```
**âœ… Expected:**
- "ðŸ”„ Starting continuous processing (every 10s)"
- "ðŸ“ˆ Processing batch at 2025-01-03..."
- Multiple processing cycles
- Terminated after 30 seconds

---

## âœ… Final Verification

### 19. Complete Status Check
```bash
uv run -s autoloader_main.py status
```
**âœ… Expected:**
- All tables showing data
- All systems healthy
- No error messages

### 20. Verify All Components
```bash
# Check configuration files exist
ls -la unity_catalog_config.yaml s3-uploader-config.yaml

# Check main script is executable
uv run -s autoloader_main.py --help
```
**âœ… Expected:**
- Configuration files exist
- Help message displays all available commands

---

## ðŸŽ¯ Success Criteria

**All of the following should be true:**

- [ ] âœ… Environment variables are set correctly
- [ ] âœ… Infrastructure setup completed without errors
- [ ] âœ… System status shows all components healthy
- [ ] âœ… Test data was created and processed successfully
- [ ] âœ… All 4 Autoloader tables contain data
- [ ] âœ… Monitoring API responds correctly
- [ ] âœ… All tests pass
- [ ] âœ… Continuous processing works (if tested)

---

## ðŸš¨ Troubleshooting

**If any step fails:**

1. **Environment Issues:** Verify all environment variables are set correctly
2. **Permission Issues:** Check Databricks token and AWS IAM role permissions
3. **Network Issues:** Verify connectivity to Databricks and AWS
4. **Port Issues:** Use different port for monitoring if 8000 is occupied

**Get help:**
```bash
uv run -s autoloader_main.py --help
uv run -s autoloader_main.py status
```

---

**ðŸŽ‰ SUCCESS: You now have a fully functional Autoloader-only sensor data pipeline!**

**Next:** Use the same commands in production with your production environment variables.
