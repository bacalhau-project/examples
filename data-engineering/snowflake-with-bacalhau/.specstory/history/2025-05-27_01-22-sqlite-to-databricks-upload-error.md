<!-- Generated by SpecStory -->

# SQLite to Databricks Upload Error (2025-05-26 18:22:51)

_**User**_

❯ uv run sqlite_to_delta_uploader.py --config config.yaml --sqlite ../sample-sensor/data/sensor_data.db
Reading inline script metadata from `sqlite_to_delta_uploader.py`
2025-05-26 18:22:43,587 INFO Using deltalake library for Databricks operations.
2025-05-26 18:22:43,588 INFO Using SQLite source table: sensor_readings
2025-05-26 18:22:43,588 INFO Using SQLite timestamp column: timestamp
2025-05-26 18:22:43,588 INFO Starting continuous upload to Databricks table 'bacalhau_catalog.sensor_data.readings' every 300 seconds, initial SQLite timestamp=2025-05-27 01:18:23.767000+00:00
2025-05-26 18:22:43,595 INFO Read 2342 new records from SQLite. Appending to Databricks table bacalhau_catalog.sensor_data.readings
2025-05-26 18:22:43,596 INFO Column 'anomaly_type' contains only null values. Casting to string type.
2025-05-26 18:22:43,597 ERROR Error in upload cycle: write_deltalake() got an unexpected keyword argument 'overwrite_schema'
Traceback (most recent call last):
  File "/Users/daaronch/code/bacalhau-examples/data-engineering/databricks-with-bacalhau/databricks-uploader/sqlite_to_delta_uploader.py", line 292, in main
    write_deltalake(
TypeError: write_deltalake() got an unexpected keyword argument 'overwrite_schema'
2025-05-26 18:22:43,597 INFO Sleeping for 300 seconds before next cycle.
^CTraceback (most recent call last):
  File "/Users/daaronch/code/bacalhau-examples/data-engineering/databricks-with-bacalhau/databricks-uploader/sqlite_to_delta_uploader.py", line 343, in <module>
    main()
  File "/Users/daaronch/code/bacalhau-examples/data-engineering/databricks-with-bacalhau/databricks-uploader/sqlite_to_delta_uploader.py", line 337, in main
    time.sleep(interval)
KeyboardInterrupt

╭─   ~/c/bacalhau-examples/data-e/databricks-with-bacalhau/databricks-uploader   main ⇡2 *1 !23 ?7 ─────────────────────────────────────────────  3s ▼
╰─❯

---

_**Assistant**_



---

Okay, I see the problem. The `write_deltalake` function from the `deltalake` library doesn't support the `overwrite_schema` argument directly in the way it's being used. I'll remove that argument.



---

```diff
                      pandas_df,
                      mode="append",
-                     overwrite_schema=True,
                      storage_options=storage_options if storage_options else None,
                      engine="rust",  # Explicitly use rust engine, though it's default
```

---

The `overwrite_schema` argument has been removed from the `write_deltalake` function call in `databricks-uploader/sqlite_to_delta_uploader.py`. This should resolve the `TypeError`.

If you need to handle schema evolution, the `deltalake` library typically manages this implicitly when appending data. If a schema change is necessary and not automatically handled, you might need to use a different mode like `overwrite` or handle schema migration explicitly, but for appending, this change should work.

Let me know if you encounter any other issues!

---

_**User**_

i'm still getting this error - ELTA_READ_TABLE_WITHOUT_COLUMNS] You are trying to read a Delta table `bacalhau_catalog`.`sensor_data`.`readings` that does not have any columns.

Write some new data with the option `mergeSchema = true` to be able to read the table.

---

_**Assistant**_



---

