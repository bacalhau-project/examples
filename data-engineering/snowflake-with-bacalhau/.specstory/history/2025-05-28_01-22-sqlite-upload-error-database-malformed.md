<!-- Generated by SpecStory -->

# SQLite Upload Error: Database Malformed (2025-05-27 18:22:04)

_**User**_

what happened here? 

Reading inline script metadata from `sqlite_to_databricks_uploader.py`
2025-05-27 18:21:43,751 INFO Detected SQLite table: sensor_readings
2025-05-27 18:21:43,751 INFO Using user-supplied timestamp column for SQLite table 'sensor_readings': timestamp
2025-05-27 18:21:43,751 INFO Starting continuous upload every 300 seconds, initial timestamp=2025-05-27 01:33:55.398000+00:00
2025-05-27 18:21:44,932 ERROR Error in upload cycle: database disk image is malformed
Traceback (most recent call last):
  File "/Users/daaronch/code/bacalhau-examples/data-engineering/databricks-with-bacalhau/databricks-uploader/sqlite_to_databricks_uploader.py", line 413, in main
    df = pd.read_sql_query(sql, conn_sqlite_data, params=[last_ts.isoformat()])
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/daaronch/code/bacalhau-examples/data-engineering/databricks-with-bacalhau/.flox/run/aarch64-darwin.databricks-with-bacalhau.dev/lib/python3.12/site-packages/pandas/io/sql.py", line 526, in read_sql_query
    return pandas_sql.read_query(
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/daaronch/code/bacalhau-examples/data-engineering/databricks-with-bacalhau/.flox/run/aarch64-darwin.databricks-with-bacalhau.dev/lib/python3.12/site-packages/pandas/io/sql.py", line 2753, in read_query
    data = self._fetchall_as_list(cursor)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/daaronch/code/bacalhau-examples/data-engineering/databricks-with-bacalhau/.flox/run/aarch64-darwin.databricks-with-bacalhau.dev/lib/python3.12/site-packages/pandas/io/sql.py", line 2768, in _fetchall_as_list
    result = cur.fetchall()
             ^^^^^^^^^^^^^^
sqlite3.DatabaseError: database disk image is malformed
^CTraceback (most recent call last):
  File "/Users/daaronch/code/bacalhau-examples/data-engineering/databricks-with-bacalhau/databricks-uploader/sqlite_to_databricks_uploader.py", line 653, in <module>
    main()
  File "/Users/daaronch/code/bacalhau-examples/data-engineering/databricks-with-bacalhau/databricks-uploader/sqlite_to_databricks_uploader.py", line 649, in main
    time.sleep(interval)
KeyboardInterrupt

╭─   ~/c/bacalhau-examples/data-e/databricks-with-bacalhau/databricks-uploader   feat/local-processing-uploader ⇡1 *1 !4 ?2 ────────────────────  7s ▼
╰─❯

---

_**Assistant**_



---

The error message `sqlite3.DatabaseError: database disk image is malformed` indicates that your SQLite database file, located at `../sample-sensor/data/sensor_data.db` (as specified in your `databricks-uploader-config.yaml`), has become corrupted.

This can happen for a variety of reasons, such as:
*   The application or system crashing while the database was being written to.
*   The database file not being closed properly after its last use.
*   Disk errors or running out of disk space.
*   An issue during file transfer if the database file was moved or copied.

Here are a few things you could try to resolve this:

1.  **Restore from a backup**: If you have a backup of the `sensor_data.db` file, restoring it would be the simplest solution.
2.  **Attempt to repair the database**: SQLite has a command-line interface that can sometimes recover corrupted databases. You can try the following:
    *   Open your terminal.
    *   Navigate to the directory containing `sensor_data.db`.
    *   Run the command `sqlite3 sensor_data.db ".recover" | sqlite3 sensor_data_recovered.db`
    *   This will attempt to recover the data and save it to a new file named `sensor_data_recovered.db`. If successful, you can then replace the corrupted file with the recovered one.
3.  **Recreate the database**: If the data in the SQLite database isn't critical or can be easily regenerated by your `sample-sensor` application, you might consider deleting the `sensor_data.db` file. The sensor application might recreate it on its next run.

Before attempting any of these, it's a good idea to make a copy of the corrupted `sensor_data.db` file, just in case.

---

_**User**_

❯ uv run -s sqlite_to_databricks_uploader.py --config ../databricks-uploader-config.yaml
Reading inline script metadata from `sqlite_to_databricks_uploader.py`
2025-05-27 18:26:08,317 INFO Detected SQLite table: sensor_readings
2025-05-27 18:26:08,317 INFO Using user-supplied timestamp column for SQLite table 'sensor_readings': timestamp
2025-05-27 18:26:08,317 INFO Starting continuous upload every 300 seconds, initial timestamp=2025-05-27 01:33:55.398000+00:00
2025-05-27 18:26:10,065 ERROR Error in upload cycle: name 'conn' is not defined
Traceback (most recent call last):
  File "/Users/daaronch/code/bacalhau-examples/data-engineering/databricks-with-bacalhau/databricks-uploader/sqlite_to_databricks_uploader.py", line 414, in main
    conn.close()
    ^^^^
NameError: name 'conn' is not defined
^CTraceback (most recent call last):
  File "/Users/daaronch/code/bacalhau-examples/data-engineering/databricks-with-bacalhau/databricks-uploader/sqlite_to_databricks_uploader.py", line 653, in <module>
    main()
  File "/Users/daaronch/code/bacalhau-examples/data-engineering/databricks-with-bacalhau/databricks-uploader/sqlite_to_databricks_uploader.py", line 649, in main
    time.sleep(interval)
KeyboardInterrupt

╭─   ~/c/bacalhau-examples/data-e/databricks-with-bacalhau/databricks-uploader   feat/local-processing-uploader ⇡1 *1 !4 ?2 ────────────────────  5s ▼
╰─❯

---

_**Assistant**_



---

Okay, I see the new error. It's a `NameError: name 'conn' is not defined` on line 414 of `sqlite_to_databricks_uploader.py`.

This happened because the SQLite connection object was named `conn_sqlite_data` when it was created, but then `conn.close()` was called.

I'll correct this by changing `conn.close()` to `conn_sqlite_data.close()`.

---

```diff
              conn_sqlite_data = sqlite3.connect(str(sqlite_path))
              df = pd.read_sql_query(sql, conn_sqlite_data, params=[last_ts.isoformat()])
-             conn.close()
+             conn_sqlite_data.close()
              if df.empty:
                  logging.info("No new records since %s", last_ts)
```

---

The `NameError` should be resolved now. The SQLite database connection was being closed using the wrong variable name. I've corrected it to use `conn_sqlite_data.close()`.

Try running your script again.

---

