{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "tags": [
          "remove_cell",
          "skip-execution"
        ]
      },
      "source": [
        "---\n",
        "sidebar_label: \"Python Scripting\"\n",
        "sidebar_position: 7\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7JcAzJVmEau1",
        "tags": []
      },
      "source": [
        "# Scripting Bacalhau with Python\n",
        "\n",
        "\n",
        "[![stars - badge-generator](https://img.shields.io/github/stars/bacalhau-project/bacalhau?style=social)](https://github.com/bacalhau-project/bacalhau)\n",
        "\n",
        "Bacalhau allows you to easily execute batch jobs via the CLI. But sometimes you need to do more than that. You might need to execute a script that requires user input, or you might need to execute a script that requires a lot of parameters. In any case, you probably want to execute your jobs in a repeatable manner.\n",
        "\n",
        "This example demonstrates a simple Python script that is able to orchestrate the execution of lots of jobs in a repeatable manner.\n",
        "\n",
        "## TD;LR\n",
        "Running Python script in Bacalhau \n",
        "\n",
        "## Prerequisite\n",
        "\n",
        "To get started, you need to install the Bacalhau client, see more information [here](https://docs.bacalhau.org/getting-started/installation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "remove_cell"
        ]
      },
      "outputs": [],
      "source": [
        "!command -v bacalhau >/dev/null 2>&1 || (export BACALHAU_INSTALL_DIR=.; curl -sL https://get.bacalhau.org/install.sh | bash)\n",
        "path=!echo $PATH\n",
        "%env PATH=./:{path[-1]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Executing Bacalhau Jobs with Python Scripts\n",
        "\n",
        "To demonstrate this example, I will use the data generated from the [ethereum analysis example](../../data-engineering/blockchain-etl/index.md). This produced a list of hashes that I will iterate over and execute a job for each one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile hashes.txt\n",
        "bafybeihvtzberlxrsz4lvzrzvpbanujmab3hr5okhxtbgv2zvonqos2l3i\n",
        "bafybeifb25fgxrzu45lsc47gldttomycqcsao22xa2gtk2ijbsa5muzegq\n",
        "bafybeig4wwwhs63ly6wbehwd7tydjjtnw425yvi2tlzt3aii3pfcj6hvoq\n",
        "bafybeievpb5q372q3w5fsezflij3wlpx6thdliz5xowimunoqushn3cwka\n",
        "bafybeih6te26iwf5kzzby2wqp67m7a5pmwilwzaciii3zipvhy64utikre\n",
        "bafybeicjd4545xph6rcyoc74wvzxyaz2vftapap64iqsp5ky6nz3f5yndm"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's run the following script. You can execute this script anywhere with `python bacalhau.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iD_DesnwEodT",
        "tags": [
          "remove_output"
        ]
      },
      "outputs": [],
      "source": [
        "%%writefile bacalhau.py\n",
        "import json, glob, os, multiprocessing, shutil, subprocess, tempfile, time\n",
        "\n",
        "# checkStatusOfJob checks the status of a Bacalhau job\n",
        "def checkStatusOfJob(job_id: str) -> str:\n",
        "    assert len(job_id) > 0\n",
        "    p = subprocess.run(\n",
        "        [\"bacalhau\", \"list\", \"--output\", \"json\", \"--id-filter\", job_id],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.PIPE,\n",
        "        text=True,\n",
        "    )\n",
        "    r = parseJobStatus(p.stdout)\n",
        "    if r == \"\":\n",
        "        print(\"job status is empty! %s\" % job_id)\n",
        "    elif r == \"Completed\":\n",
        "        print(\"job completed: %s\" % job_id)\n",
        "    else:\n",
        "        print(\"job not completed: %s - %s\" % (job_id, r))\n",
        "\n",
        "    return r\n",
        "\n",
        "\n",
        "# submitJob submits a job to the Bacalhau network\n",
        "def submitJob(cid: str) -> str:\n",
        "    assert len(cid) > 0\n",
        "    p = subprocess.run(\n",
        "        [\n",
        "            \"bacalhau\",\n",
        "            \"docker\",\n",
        "            \"run\",\n",
        "            \"--id-only\",\n",
        "            \"--wait=false\",\n",
        "            \"--input\",\n",
        "            \"ipfs://\" + cid + \":/inputs/data.tar.gz\",\n",
        "            \"ghcr.io/bacalhau-project/examples/blockchain-etl:0.0.6\",\n",
        "        ],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.PIPE,\n",
        "        text=True,\n",
        "    )\n",
        "    if p.returncode != 0:\n",
        "        print(\"failed (%d) job: %s\" % (p.returncode, p.stdout))\n",
        "    job_id = p.stdout.strip()\n",
        "    print(\"job submitted: %s\" % job_id)\n",
        "\n",
        "    return job_id\n",
        "\n",
        "\n",
        "# getResultsFromJob gets the results from a Bacalhau job\n",
        "def getResultsFromJob(job_id: str) -> str:\n",
        "    assert len(job_id) > 0\n",
        "    temp_dir = tempfile.mkdtemp()\n",
        "    print(\"getting results for job: %s\" % job_id)\n",
        "    for i in range(0, 5): # try 5 times\n",
        "        p = subprocess.run(\n",
        "            [\n",
        "                \"bacalhau\",\n",
        "                \"get\",\n",
        "                \"--output-dir\",\n",
        "                temp_dir,\n",
        "                job_id,\n",
        "            ],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            text=True,\n",
        "        )\n",
        "        if p.returncode == 0:\n",
        "            break\n",
        "        else:\n",
        "            print(\"failed (exit %d) to get job: %s\" % (p.returncode, p.stdout))\n",
        "\n",
        "    return temp_dir\n",
        "\n",
        "\n",
        "# parseJobStatus parses the status of a Bacalhau job\n",
        "def parseJobStatus(result: str) -> str:\n",
        "    if len(result) == 0:\n",
        "        return \"\"\n",
        "    r = json.loads(result)\n",
        "    if len(r) > 0:\n",
        "        return r[0][\"State\"][\"State\"]\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "# parseHashes splits lines from a text file into a list\n",
        "def parseHashes(filename: str) -> list:\n",
        "    assert os.path.exists(filename)\n",
        "    with open(filename, \"r\") as f:\n",
        "        hashes = f.read().splitlines()\n",
        "    return hashes\n",
        "\n",
        "\n",
        "def main(file: str, num_files: int = -1):\n",
        "    # Use multiprocessing to work in parallel\n",
        "    count = multiprocessing.cpu_count()\n",
        "    with multiprocessing.Pool(processes=count) as pool:\n",
        "        hashes = parseHashes(file)[:num_files]\n",
        "        print(\"submitting %d jobs\" % len(hashes))\n",
        "        job_ids = pool.map(submitJob, hashes)\n",
        "        assert len(job_ids) == len(hashes)\n",
        "\n",
        "        print(\"waiting for jobs to complete...\")\n",
        "        while True:\n",
        "            job_statuses = pool.map(checkStatusOfJob, job_ids)\n",
        "            total_finished = sum(map(lambda x: x == \"Completed\", job_statuses))\n",
        "            if total_finished >= len(job_ids):\n",
        "                break\n",
        "            print(\"%d/%d jobs completed\" % (total_finished, len(job_ids)))\n",
        "            time.sleep(2)\n",
        "\n",
        "        print(\"all jobs completed, saving results...\")\n",
        "        results = pool.map(getResultsFromJob, job_ids)\n",
        "        print(\"finished saving results\")\n",
        "\n",
        "        # Do something with the results\n",
        "        shutil.rmtree(\"results\", ignore_errors=True)\n",
        "        os.makedirs(\"results\", exist_ok=True)\n",
        "        for r in results:\n",
        "            path = os.path.join(r, \"outputs\", \"*.csv\")\n",
        "            csv_file = glob.glob(path)\n",
        "            for f in csv_file:\n",
        "                print(\"moving %s to results\" % f)\n",
        "                shutil.move(f, \"results\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(\"hashes.txt\", 10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code has a few interesting features:\n",
        "* Change the value in the `main` call to change the number of jobs to execute\n",
        "* Because all jobs complete at different times, there's a loop to check that all jobs have completed before downloading the results -- if you don't do this you'll likely see an error when trying to download the results\n",
        "* When downloading the results, the IPFS get often times out, so I wrapped that in a loop\n",
        "\n",
        "Let's run it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "python bacalhau.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hopefully the results directory contains all the combined results from the jobs we just executed. Here's we're expecting to see csv files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "ls -l results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Success! We've now executed a bunch of jobs in parallel using Python. This is a great way to execute lots of jobs in a repeatable manner. You can alter the file above for your purposes.\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "You might also be interested in the following examples:\n",
        "\n",
        "* [Analysing Ethereum Data with Python](../../data-engineering/blockchain-etl/index.md)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPMkD4lMPwZFqll4/eh5uBe",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit ('3.9.7')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "21fd917facdca5c02b7d24e32528f1b4e6711465b0262edbfffba943391e1222"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
