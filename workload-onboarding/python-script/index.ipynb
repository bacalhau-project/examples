{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "tags": [
          "remove_cell",
          "skip-execution"
        ]
      },
      "source": [
        "---\n",
        "sidebar_label: \"Python - Scripting\"\n",
        "sidebar_position: 7\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7JcAzJVmEau1",
        "tags": []
      },
      "source": [
        "# Scripting Bacalhau with Python\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bacalhau-project/examples/blob/main/workload-onboarding/python-script/index.ipynb)\n",
        "[![Open In Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/bacalhau-project/examples/HEAD?labpath=workload-onboarding/python-script/index.ipynb)\n",
        "[![stars - badge-generator](https://img.shields.io/github/stars/bacalhau-project/bacalhau?style=social)](https://github.com/bacalhau-project/bacalhau)\n",
        "\n",
        "Bacalhau allows you to easily execute batch jobs via the CLI. But sometimes you need to do more than that. You might need to execute a script that requires user input, or you might need to execute a script that requires a lot of parameters. In any case, you probably want to execute your jobs in a repeatable manner.\n",
        "\n",
        "This example demonstrates a simple Python script that is able to orchestrate the execution of lots of jobs in a repeatable manner.\n",
        "\n",
        "## TD;LR\n",
        "Running Python script in Bacalhau \n",
        "\n",
        "## Prerequisite\n",
        "\n",
        "To get started, you need to install the Bacalhau client, see more information [here](https://docs.bacalhau.org/getting-started/installation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "tags": [
          "remove_cell"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: PATH=./:./:/Users/phil/.pyenv/versions/3.9.7/bin:/opt/homebrew/Caskroom/google-cloud-sdk/latest/google-cloud-sdk/bin:/Users/phil/.gvm/bin:/opt/homebrew/opt/findutils/libexec/gnubin:/opt/homebrew/opt/coreutils/libexec/gnubin:/opt/homebrew/Caskroom/google-cloud-sdk/latest/google-cloud-sdk/bin:/Users/phil/.pyenv/shims:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/usr/local/MacGPG2/bin:/Users/phil/.nexustools:/opt/homebrew/Caskroom/google-cloud-sdk/latest/google-cloud-sdk/bin:/Users/phil/.gvm/bin:/opt/homebrew/opt/findutils/libexec/gnubin:/opt/homebrew/opt/coreutils/libexec/gnubin:/opt/homebrew/Caskroom/google-cloud-sdk/latest/google-cloud-sdk/bin:/Users/phil/.pyenv/shims:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/usr/local/MacGPG2/bin:/Users/phil/.nexustools\n"
          ]
        }
      ],
      "source": [
        "!command -v bacalhau >/dev/null 2>&1 || (export BACALHAU_INSTALL_DIR=.; curl -sL https://get.bacalhau.org/install.sh | bash)\n",
        "path=!echo $PATH\n",
        "%env PATH=./:{path[-1]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Executing Bacalhau Jobs with Python Scripts\n",
        "\n",
        "To demonstrate this example, I will use the data generated from the [ethereum analysis example](../../data-engineering/blockchain-etl/index.md). This produced a list of hashes that I will iterate over and execute a job for each one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile hashes.txt\n",
        "bafybeihvtzberlxrsz4lvzrzvpbanujmab3hr5okhxtbgv2zvonqos2l3i\n",
        "bafybeifb25fgxrzu45lsc47gldttomycqcsao22xa2gtk2ijbsa5muzegq\n",
        "bafybeig4wwwhs63ly6wbehwd7tydjjtnw425yvi2tlzt3aii3pfcj6hvoq\n",
        "bafybeievpb5q372q3w5fsezflij3wlpx6thdliz5xowimunoqushn3cwka\n",
        "bafybeih6te26iwf5kzzby2wqp67m7a5pmwilwzaciii3zipvhy64utikre\n",
        "bafybeicjd4545xph6rcyoc74wvzxyaz2vftapap64iqsp5ky6nz3f5yndm"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's run the following script. You can execute this script anywhere with `python bacalhau.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iD_DesnwEodT",
        "tags": [
          "remove_output"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting bacalhau.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile bacalhau.py\n",
        "import json, glob, os, multiprocessing, shutil, subprocess, tempfile, time\n",
        "\n",
        "# checkStatusOfJob checks the status of a Bacalhau job\n",
        "def checkStatusOfJob(job_id: str) -> str:\n",
        "    assert len(job_id) > 0\n",
        "    p = subprocess.run(\n",
        "        [\"bacalhau\", \"list\", \"--output\", \"json\", \"--id-filter\", job_id],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.PIPE,\n",
        "        text=True,\n",
        "    )\n",
        "    r = parseJobStatus(p.stdout)\n",
        "    if r == \"\":\n",
        "        print(\"job status is empty! %s\" % job_id)\n",
        "    elif r == \"Completed\":\n",
        "        print(\"job completed: %s\" % job_id)\n",
        "    else:\n",
        "        print(\"job not completed: %s - %s\" % (job_id, r))\n",
        "\n",
        "    return r\n",
        "\n",
        "\n",
        "# submitJob submits a job to the Bacalhau network\n",
        "def submitJob(cid: str) -> str:\n",
        "    assert len(cid) > 0\n",
        "    p = subprocess.run(\n",
        "        [\n",
        "            \"bacalhau\",\n",
        "            \"docker\",\n",
        "            \"run\",\n",
        "            \"--id-only\",\n",
        "            \"--wait=false\",\n",
        "            \"--input-volumes\",\n",
        "            cid + \":/inputs/data.tar.gz\",\n",
        "            \"ghcr.io/bacalhau-project/examples/blockchain-etl:0.0.6\",\n",
        "        ],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.PIPE,\n",
        "        text=True,\n",
        "    )\n",
        "    if p.returncode != 0:\n",
        "        print(\"failed (%d) job: %s\" % (p.returncode, p.stdout))\n",
        "    job_id = p.stdout.strip()\n",
        "    print(\"job submitted: %s\" % job_id)\n",
        "\n",
        "    return job_id\n",
        "\n",
        "\n",
        "# getResultsFromJob gets the results from a Bacalhau job\n",
        "def getResultsFromJob(job_id: str) -> str:\n",
        "    assert len(job_id) > 0\n",
        "    temp_dir = tempfile.mkdtemp()\n",
        "    print(\"getting results for job: %s\" % job_id)\n",
        "    for i in range(0, 5): # try 5 times\n",
        "        p = subprocess.run(\n",
        "            [\n",
        "                \"bacalhau\",\n",
        "                \"get\",\n",
        "                \"--output-dir\",\n",
        "                temp_dir,\n",
        "                job_id,\n",
        "            ],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            text=True,\n",
        "        )\n",
        "        if p.returncode == 0:\n",
        "            break\n",
        "        else:\n",
        "            print(\"failed (exit %d) to get job: %s\" % (p.returncode, p.stdout))\n",
        "\n",
        "    return temp_dir\n",
        "\n",
        "\n",
        "# parseJobStatus parses the status of a Bacalhau job\n",
        "def parseJobStatus(result: str) -> str:\n",
        "    if len(result) == 0:\n",
        "        return \"\"\n",
        "    r = json.loads(result)\n",
        "    if len(r) > 0:\n",
        "        for _, v in r[0][\"Status\"][\"JobState\"][\"Nodes\"].items():\n",
        "            state = v[\"Shards\"][\"0\"][\"State\"]\n",
        "            if state == \"Completed\":\n",
        "                return state\n",
        "        for _, v in r[0][\"Status\"][\"JobState\"][\"Nodes\"].items():\n",
        "            state = v[\"Shards\"][\"0\"][\"State\"]\n",
        "            if state != \"Cancelled\":\n",
        "                return state\n",
        "        return \"Error\"\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "# parseHashes splits lines from a text file into a list\n",
        "def parseHashes(filename: str) -> list:\n",
        "    assert os.path.exists(filename)\n",
        "    with open(filename, \"r\") as f:\n",
        "        hashes = f.read().splitlines()\n",
        "    return hashes\n",
        "\n",
        "\n",
        "def main(file: str, num_files: int = -1):\n",
        "    # Use multiprocessing to work in parallel\n",
        "    count = multiprocessing.cpu_count()\n",
        "    with multiprocessing.Pool(processes=count) as pool:\n",
        "        hashes = parseHashes(file)[:num_files]\n",
        "        print(\"submitting %d jobs\" % len(hashes))\n",
        "        job_ids = pool.map(submitJob, hashes)\n",
        "        assert len(job_ids) == len(hashes)\n",
        "\n",
        "        print(\"waiting for jobs to complete...\")\n",
        "        while True:\n",
        "            job_statuses = pool.map(checkStatusOfJob, job_ids)\n",
        "            total_finished = sum(map(lambda x: x == \"Completed\", job_statuses))\n",
        "            if total_finished >= len(job_ids):\n",
        "                break\n",
        "            print(\"%d/%d jobs completed\" % (total_finished, len(job_ids)))\n",
        "            time.sleep(2)\n",
        "\n",
        "        print(\"all jobs completed, saving results...\")\n",
        "        results = pool.map(getResultsFromJob, job_ids)\n",
        "        print(\"finished saving results\")\n",
        "\n",
        "        # Do something with the results\n",
        "        shutil.rmtree(\"results\", ignore_errors=True)\n",
        "        os.makedirs(\"results\", exist_ok=True)\n",
        "        for r in results:\n",
        "            path = os.path.join(r, \"combined_results\", \"outputs\", \"*.csv\")\n",
        "            csv_file = glob.glob(path)\n",
        "            for f in csv_file:\n",
        "                print(\"moving %s to results\" % f)\n",
        "                shutil.move(f, \"results\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(\"hashes.txt\", 10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code has a few interesting features:\n",
        "* Change the value in the `main` call to change the number of jobs to execute\n",
        "* Because all jobs complete at different times, there's a loop to check that all jobs have completed before downloading the results -- if you don't do this you'll likely see an error when trying to download the results\n",
        "* When downloading the results, the IPFS get often times out, so I wrapped that in a loop\n",
        "\n",
        "Let's run it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "submitting 10 jobs\n",
            "job submitted: dead536c-286a-4632-9105-d4fdf81b9863\n",
            "job submitted: 42dabff1-4116-46df-9be7-5b2fc015a3fe\n",
            "job submitted: 82f1f934-8acd-4e56-919f-f09928323b19\n",
            "job submitted: 3cbc3334-d3a2-4980-8bad-4e4347814040\n",
            "job submitted: 2d2c3b70-2739-49b0-b8af-05236a836630\n",
            "job submitted: 7289b1ee-5863-4274-ae0f-4db0ac2dd3b3\n",
            "job submitted: fb5ddaa5-d0ca-4c77-8bb3-a5af78a327f4\n",
            "job submitted: c399b0c9-0f9c-4d74-afc0-f8cfcecc8d02\n",
            "job submitted: e8d83d77-ea16-41fb-8c20-7e2e809a187b\n",
            "job submitted: b6b49a8b-6145-4728-a16b-f3e657464e67\n",
            "waiting for jobs to complete...\n",
            "job not completed: 42dabff1-4116-46df-9be7-5b2fc015a3fe - Waiting\n",
            "job not completed: fb5ddaa5-d0ca-4c77-8bb3-a5af78a327f4 - Waiting\n",
            "job not completed: 82f1f934-8acd-4e56-919f-f09928323b19 - Waiting\n",
            "job not completed: 7289b1ee-5863-4274-ae0f-4db0ac2dd3b3 - Waiting\n",
            "job not completed: dead536c-286a-4632-9105-d4fdf81b9863 - Waiting\n",
            "job not completed: 2d2c3b70-2739-49b0-b8af-05236a836630 - Waiting\n",
            "job not completed: 3cbc3334-d3a2-4980-8bad-4e4347814040 - Waiting\n",
            "job not completed: c399b0c9-0f9c-4d74-afc0-f8cfcecc8d02 - Waiting\n",
            "job not completed: e8d83d77-ea16-41fb-8c20-7e2e809a187b - Waiting\n",
            "job not completed: b6b49a8b-6145-4728-a16b-f3e657464e67 - Waiting\n",
            "0/10 jobs completed\n",
            "job not completed: 7289b1ee-5863-4274-ae0f-4db0ac2dd3b3 - Waiting\n",
            "job not completed: c399b0c9-0f9c-4d74-afc0-f8cfcecc8d02 - Waiting\n",
            "job not completed: fb5ddaa5-d0ca-4c77-8bb3-a5af78a327f4 - Waiting\n",
            "job not completed: 2d2c3b70-2739-49b0-b8af-05236a836630 - Waiting\n",
            "job not completed: 82f1f934-8acd-4e56-919f-f09928323b19 - Waiting\n",
            "job not completed: dead536c-286a-4632-9105-d4fdf81b9863 - Waiting\n",
            "job not completed: 3cbc3334-d3a2-4980-8bad-4e4347814040 - Waiting\n",
            "job completed: 42dabff1-4116-46df-9be7-5b2fc015a3fe\n",
            "job not completed: e8d83d77-ea16-41fb-8c20-7e2e809a187b - Waiting\n",
            "job not completed: b6b49a8b-6145-4728-a16b-f3e657464e67 - Waiting\n",
            "1/10 jobs completed\n",
            "job completed: 42dabff1-4116-46df-9be7-5b2fc015a3fe\n",
            "job completed: dead536c-286a-4632-9105-d4fdf81b9863\n",
            "job completed: c399b0c9-0f9c-4d74-afc0-f8cfcecc8d02\n",
            "job completed: 3cbc3334-d3a2-4980-8bad-4e4347814040\n",
            "job completed: 82f1f934-8acd-4e56-919f-f09928323b19\n",
            "job completed: 2d2c3b70-2739-49b0-b8af-05236a836630\n",
            "job completed: fb5ddaa5-d0ca-4c77-8bb3-a5af78a327f4\n",
            "job completed: 7289b1ee-5863-4274-ae0f-4db0ac2dd3b3\n",
            "job completed: b6b49a8b-6145-4728-a16b-f3e657464e67\n",
            "job completed: e8d83d77-ea16-41fb-8c20-7e2e809a187b\n",
            "all jobs completed, saving results...\n",
            "getting results for job: dead536c-286a-4632-9105-d4fdf81b9863\n",
            "getting results for job: 82f1f934-8acd-4e56-919f-f09928323b19\n",
            "getting results for job: 2d2c3b70-2739-49b0-b8af-05236a836630\n",
            "getting results for job: 3cbc3334-d3a2-4980-8bad-4e4347814040\n",
            "getting results for job: 42dabff1-4116-46df-9be7-5b2fc015a3fe\n",
            "getting results for job: c399b0c9-0f9c-4d74-afc0-f8cfcecc8d02\n",
            "getting results for job: 7289b1ee-5863-4274-ae0f-4db0ac2dd3b3\n",
            "getting results for job: fb5ddaa5-d0ca-4c77-8bb3-a5af78a327f4\n",
            "getting results for job: e8d83d77-ea16-41fb-8c20-7e2e809a187b\n",
            "getting results for job: b6b49a8b-6145-4728-a16b-f3e657464e67\n",
            "finished saving results\n",
            "moving /var/folders/kr/pl4p96k11b55hp5_p9l_t8kr0000gn/T/tmpacgvy7wu/combined_results/outputs/transactions_00000000_00049999.csv to results\n",
            "moving /var/folders/kr/pl4p96k11b55hp5_p9l_t8kr0000gn/T/tmp05iwhtpp/combined_results/outputs/transactions_00050000_00099999.csv to results\n",
            "moving /var/folders/kr/pl4p96k11b55hp5_p9l_t8kr0000gn/T/tmp6t87xlzc/combined_results/outputs/transactions_00100000_00149999.csv to results\n",
            "moving /var/folders/kr/pl4p96k11b55hp5_p9l_t8kr0000gn/T/tmp75fer_gp/combined_results/outputs/transactions_00150000_00199999.csv to results\n",
            "moving /var/folders/kr/pl4p96k11b55hp5_p9l_t8kr0000gn/T/tmphpikbnbj/combined_results/outputs/transactions_00200000_00249999.csv to results\n",
            "moving /var/folders/kr/pl4p96k11b55hp5_p9l_t8kr0000gn/T/tmp8951a72p/combined_results/outputs/transactions_00250000_00299999.csv to results\n",
            "moving /var/folders/kr/pl4p96k11b55hp5_p9l_t8kr0000gn/T/tmp9baglzje/combined_results/outputs/transactions_00300000_00349999.csv to results\n",
            "moving /var/folders/kr/pl4p96k11b55hp5_p9l_t8kr0000gn/T/tmpa13amd3g/combined_results/outputs/transactions_00350000_00399999.csv to results\n",
            "moving /var/folders/kr/pl4p96k11b55hp5_p9l_t8kr0000gn/T/tmp4v6lqc80/combined_results/outputs/transactions_00400000_00449999.csv to results\n",
            "moving /var/folders/kr/pl4p96k11b55hp5_p9l_t8kr0000gn/T/tmpqgm5ka1s/combined_results/outputs/transactions_00450000_00499999.csv to results\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "python bacalhau.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hopefully the results directory contains all the combined results from the jobs we just executed. Here's we're expecting to see csv files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 40\n",
            "-rw-r--r-- 3 phil staff  55 Jan 12 13:48 transactions_00000000_00049999.csv\n",
            "-rw-r--r-- 3 phil staff 387 Jan 12 13:49 transactions_00050000_00099999.csv\n",
            "-rw-r--r-- 3 phil staff 388 Jan 12 13:50 transactions_00100000_00149999.csv\n",
            "-rw-r--r-- 3 phil staff 426 Jan 12 13:48 transactions_00150000_00199999.csv\n",
            "-rw-r--r-- 3 phil staff 393 Jan 12 13:49 transactions_00200000_00249999.csv\n",
            "-rw-r--r-- 3 phil staff 384 Jan 12 13:48 transactions_00250000_00299999.csv\n",
            "-rw-r--r-- 3 phil staff 421 Jan 12 13:48 transactions_00300000_00349999.csv\n",
            "-rw-r--r-- 3 phil staff 390 Jan 12 13:48 transactions_00350000_00399999.csv\n",
            "-rw-r--r-- 3 phil staff 347 Jan 12 13:48 transactions_00400000_00449999.csv\n",
            "-rw-r--r-- 3 phil staff 386 Jan 12 13:48 transactions_00450000_00499999.csv\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "ls -l results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Success! We've now executed a bunch of jobs in parallel using Python. This is a great way to execute lots of jobs in a repeatable manner. You can alter the file above for your purposes.\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "You might also be interested in the following examples:\n",
        "\n",
        "* [Analysing Ethereum Data with Python](../../data-engineering/blockchain-etl/index.md)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPMkD4lMPwZFqll4/eh5uBe",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit ('3.9.7')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "21fd917facdca5c02b7d24e32528f1b4e6711465b0262edbfffba943391e1222"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
