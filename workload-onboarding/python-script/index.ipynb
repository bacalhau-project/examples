{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "tags": [
          "remove_cell",
          "skip-execution"
        ]
      },
      "source": [
        "---\n",
        "sidebar_label: \"Python - Scripting\"\n",
        "sidebar_position: 4\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JcAzJVmEau1",
        "tags": []
      },
      "source": [
        "# Scripting Bacalhau with Python\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bacalhau-project/examples/blob/main/workload-onboarding/python-script/index.ipynb)\n",
        "[![Open In Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/bacalhau-project/examples/HEAD?labpath=workload-onboarding/python-script/index.ipynb)\n",
        "\n",
        "Bacalhau allows you to easily execute batch jobs via the CLI. But sometimes you need to do more than that. You might need to execute a script that requires user input, or you might need to execute a script that requires a lot of parameters. In any case, you probably want to execute your jobs in a repeatable manner.\n",
        "\n",
        "This example demonstrates a simple Python script that is able to orchestrate the execution of lots of jobs in a repeatable manner.\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "* Python\n",
        "* The Bacalhau client - [Installation instructions](https://docs.bacalhau.org/getting-started/installation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "tags": [
          "remove_cell"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/phil/.zshenv:.:1: no such file or directory: /Users/phil/.cargo/env\n",
            "env: PATH=./:/Users/phil/.pyenv/versions/3.9.7/bin:/opt/homebrew/Caskroom/google-cloud-sdk/latest/google-cloud-sdk/bin:/Users/phil/.gvm/bin:/opt/homebrew/opt/findutils/libexec/gnubin:/opt/homebrew/opt/coreutils/libexec/gnubin:/opt/homebrew/Caskroom/google-cloud-sdk/latest/google-cloud-sdk/bin:/Users/phil/.pyenv/shims:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/usr/local/MacGPG2/bin:/Users/phil/source/bacalhau-project/examples/workload-onboarding/rust-wasm/./bin:/Users/phil/.nexustools\n"
          ]
        }
      ],
      "source": [
        "!command -v bacalhau >/dev/null 2>&1 || (export BACALHAU_INSTALL_DIR=.; curl -sL https://get.bacalhau.org/install.sh | bash)\n",
        "path=!echo $PATH\n",
        "%env PATH=./:{path[-1]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Executing Bacalhau Jobs with Python Scripts\n",
        "\n",
        "To demonstrate this example, I will use the data generated from the [ethereum analysis example](../../data-engineering/blockchain-etl/index.md). This produced a list of hashes that I will iterate over and execute a job for each one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting hashes.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile hashes.txt\n",
        "bafybeihvtzberlxrsz4lvzrzvpbanujmab3hr5okhxtbgv2zvonqos2l3i\n",
        "bafybeifb25fgxrzu45lsc47gldttomycqcsao22xa2gtk2ijbsa5muzegq\n",
        "bafybeig4wwwhs63ly6wbehwd7tydjjtnw425yvi2tlzt3aii3pfcj6hvoq\n",
        "bafybeievpb5q372q3w5fsezflij3wlpx6thdliz5xowimunoqushn3cwka\n",
        "bafybeih6te26iwf5kzzby2wqp67m7a5pmwilwzaciii3zipvhy64utikre\n",
        "bafybeicjd4545xph6rcyoc74wvzxyaz2vftapap64iqsp5ky6nz3f5yndm\n",
        "bafybeicgo3iofo3sw73wenc3nkdhi263yytjnds5cxjwvypwekbz4sk7ra\n",
        "bafybeihvep5xsvxm44lngmmeysihsopcuvcr34an4idz45ixl5slsqzy3y\n",
        "bafybeigmt2zwzrbzwb4q2kt2ihlv34ntjjwujftvabrftyccwzwdypama4\n",
        "bafybeiciwui7sw3zqkvp4d55p4woq4xgjlstrp3mzxl66ab5ih5vmeozci\n",
        "bafybeicpmotdsj2ambf666b2jkzp2gvg6tadr6acxqw2tmdlmsruuggbbu\n",
        "bafybeigefo3esovbveavllgv5wiheu5w6cnfo72jxe6vmfweco5eq5sfty\n",
        "bafybeigvajsumnfwuv7lp7yhr2sr5vrk3bmmuhhnaz53waa2jqv3kgkvsu\n",
        "bafybeih2xg2n7ytlunvqxwqlqo5l3daykuykyvhgehoa2arot6dmorstmq\n",
        "bafybeihnmq2ltuolnlthb757teihwvvw7wophoag2ihnva43afbeqdtgi4\n",
        "bafybeibb34hzu6z2xgo6nhrplt3xntpnucthqlawe3pmzgxccppbxrpudy\n",
        "bafybeigny33b4g6gf2hrqzzkfbroprqrimjl5gmb3mnsqu655pbbny6tou\n",
        "bafybeifgqjvmzbtz427bne7af5tbndmvniabaex77us6l637gqtb2iwlwq\n",
        "bafybeibryqj62l45pxjhdyvgdc44p3suhvt4xdqc5jpx474gpykxwgnw2e\n",
        "bafybeidme3fkigdjaifkjfbwn76jk3fcqdogpzebtotce6ygphlujaecla\n",
        "bafybeig7myc3eg3h2g5mk2co7ybte4qsuremflrjneer6xk3pghjwmcwbi\n",
        "bafybeic3x2r5rrd3fdpdqeqax4bszcciwepvbpjl7xdv6mkwubyqizw5te\n",
        "bafybeihxutvxg3bw7fbwohq4gvncrk3hngkisrtkp52cu7qu7tfcuvktnq\n",
        "bafybeicumr67jkyarg5lspqi2w4zqopvgii5dgdbe5vtbbq53mbyftduxy\n",
        "bafybeiecn2cdvefvdlczhz6i4afbkabf5pe5yqrcsgdvlw5smme2tw7em4\n",
        "bafybeiaxh7dhg4krgkil5wqrv5kdsc3oewwy6ym4n3545ipmzqmxaxrqf4\n",
        "bafybeiclcqfzinrmo3adr4lg7sf255faioxjfsolcdko3i4x7opx7xrqii\n",
        "bafybeicjmeul7c2dxhmaudawum4ziwfgfkvbgthgtliggfut5tsc77dx7q\n",
        "bafybeialziupik7csmhfxnhuss5vrw37kmte7rmboqovp4cpq5hj4insda\n",
        "bafybeid7ecwdrw7pb3fnkokq5adybum6s5ok3yi2lw4m3edjpuy65zm4ji\n",
        "bafybeibuxwnl5ogs4pwa32xriqhch24zbrw44rp22hrly4t6roh6rz7j4m\n",
        "bafybeicxvy47jpvv3fi5umjatem5pxabfrbkzxiho7efu6mpidjpatte54\n",
        "bafybeifynb4mpqrbsjbeqtxpbuf6y4frrtjrc4tm7cnmmui7gbjkckszrq\n",
        "bafybeidcgnbhguyfaahkoqbyy2z525d3qfzdtbjuk4e75wkdbnkcafvjei\n",
        "bafybeiefc67s6hpydnsqdgypbunroqwkij5j26sfmc7are7yxvg45uuh7i\n",
        "bafybeiefwjy3o42ovkssnm7iihbog46k5grk3gobvvkzrqvof7p6xbgowi\n",
        "bafybeihpydd3ivtza2ql5clatm5fy7ocych7t4czu46sbc6c2ykrbwk5uu\n",
        "bafybeiet7222lqfmzogur3zlxqavlnd3lt3qryw5yi5rhuiqeqg4w7c3qu\n",
        "bafybeihwomd4ygoydvj5kh24wfwk5kszmst5vz44zkl6yibjargttv7sly\n",
        "bafybeidbjt2ckr4oooio3jsfk76r3bsaza5trjvt7u36slhha5ksoc5gv4\n",
        "bafybeifyjrmopgtfmswq7b4pfscni46doy3g3z6vi5rrgpozc6duebpmuy\n",
        "bafybeidsrowz46yt62zs64q2mhirlc3rsmctmi3tluorsts53vppdqjj7e\n",
        "bafybeiggntql57bw24bw6hkp2yqd3qlyp5oxowo6q26wsshxopfdnzsxhq\n",
        "bafybeidguz36u6wakx4e5ewuhslsfsjmk5eff5q7un2vpkrcu7cg5aaqf4\n",
        "bafybeiaypwu2b45iunbqnfk2g7bku3nfqveuqp4vlmmwj7o7liyys42uai\n",
        "bafybeicaahv7xvia7xojgiecljo2ddrvryzh2af7rb3qqbg5a257da5p2y\n",
        "bafybeibgeiijr74rcliwal3e7tujybigzqr6jmtchqrcjdo75trm2ptb4e\n",
        "bafybeiba3nrd43ylnedipuq2uoowd4blghpw2z7r4agondfinladcsxlku\n",
        "bafybeif3semzitjbxg5lzwmnjmlsrvc7y5htekwqtnhmfi4wxywtj5lgoe\n",
        "bafybeiedmsig5uj7rgarsjans2ad5kcb4w4g5iurbryqn62jy5qap4qq2a\n",
        "bafybeidyz34bcd3k6nxl7jbjjgceg5eu3szbrbgusnyn7vfl7facpecsce\n",
        "bafybeigmq5gch72q3qpk4nipssh7g7msk6jpzns2d6xmpusahkt2lu5m4y\n",
        "bafybeicjzoypdmmdt6k54wzotr5xhpzwbgd3c4oqg6mj4qukgvxvdrvzye\n",
        "bafybeien55egngdpfvrsxr2jmkewdyha72ju7qaaeiydz2f5rny7drgzta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's run the following script. There's a fair bit of code, but basically there's three core functions to submit, check the status of, and download the results from, a job. Then the main function wraps all of that in a `multiprocessing` pool to execute the jobs in parallel. Feel free to copy this code and save it to your local machine as `bacalhau.py`. Then you can execute this script anywhere with `python bacalhau.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "iD_DesnwEodT",
        "tags": [
          "remove_output"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting bacalhau.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile bacalhau.py\n",
        "import json, glob, os, multiprocessing, shutil, subprocess, tempfile, time\n",
        "\n",
        "# checkStatusOfJob checks the status of a Bacalhau job\n",
        "def checkStatusOfJob(job_id: str) -> str:\n",
        "    assert len(job_id) > 0\n",
        "    p = subprocess.run(\n",
        "        [\"bacalhau\", \"list\", \"--output\", \"json\", \"--id-filter\", job_id],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.PIPE,\n",
        "        text=True,\n",
        "    )\n",
        "    r = parseJobStatus(p.stdout)\n",
        "    if r == \"\":\n",
        "        print(\"job status is empty! %s\" % job_id)\n",
        "    elif r == \"Completed\":\n",
        "        print(\"job completed: %s\" % job_id)\n",
        "    else:\n",
        "        print(\"job not completed: %s - %s\" % (job_id, r))\n",
        "\n",
        "    return r\n",
        "\n",
        "\n",
        "# submitJob submits a job to the Bacalhau network\n",
        "def submitJob(cid: str) -> str:\n",
        "    assert len(cid) > 0\n",
        "    p = subprocess.run(\n",
        "        [\n",
        "            \"bacalhau\",\n",
        "            \"docker\",\n",
        "            \"run\",\n",
        "            \"--id-only\",\n",
        "            \"--wait=false\",\n",
        "            \"--input-volumes\",\n",
        "            cid + \":/inputs/data.tar.gz\",\n",
        "            \"ghcr.io/bacalhau-project/examples/blockchain-etl:0.0.6\",\n",
        "        ],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.PIPE,\n",
        "        text=True,\n",
        "    )\n",
        "    if p.returncode != 0:\n",
        "        print(\"failed (%d) job: %s\" % (p.returncode, p.stdout))\n",
        "    job_id = p.stdout.strip()\n",
        "    print(\"job submitted: %s\" % job_id)\n",
        "\n",
        "    return job_id\n",
        "\n",
        "\n",
        "# getResultsFromJob gets the results from a Bacalhau job\n",
        "def getResultsFromJob(job_id: str) -> str:\n",
        "    assert len(job_id) > 0\n",
        "    temp_dir = tempfile.mkdtemp()\n",
        "    print(\"getting results for job: %s\" % job_id)\n",
        "    for i in range(0, 5): # try 5 times\n",
        "        p = subprocess.run(\n",
        "            [\n",
        "                \"bacalhau\",\n",
        "                \"get\",\n",
        "                \"--output-dir\",\n",
        "                temp_dir,\n",
        "                job_id,\n",
        "            ],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            text=True,\n",
        "        )\n",
        "        if p.returncode == 0:\n",
        "            break\n",
        "        else:\n",
        "            print(\"failed (exit %d) to get job: %s\" % (p.returncode, p.stdout))\n",
        "\n",
        "    return temp_dir\n",
        "\n",
        "\n",
        "# parseJobStatus parses the status of a Bacalhau job\n",
        "def parseJobStatus(result: str) -> str:\n",
        "    if len(result) == 0:\n",
        "        return \"\"\n",
        "    r = json.loads(result)\n",
        "    if len(r) > 0:\n",
        "        for _, v in r[0][\"JobState\"][\"Nodes\"].items():\n",
        "            state = v[\"Shards\"][\"0\"][\"State\"]\n",
        "            if state == \"Completed\":\n",
        "                return state\n",
        "        for _, v in r[0][\"JobState\"][\"Nodes\"].items():\n",
        "            state = v[\"Shards\"][\"0\"][\"State\"]\n",
        "            if state != \"Cancelled\":\n",
        "                return state\n",
        "        return \"Error\"\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "# parseHashes splits lines from a text file into a list\n",
        "def parseHashes(filename: str) -> list:\n",
        "    assert os.path.exists(filename)\n",
        "    with open(filename, \"r\") as f:\n",
        "        hashes = f.read().splitlines()\n",
        "    return hashes\n",
        "\n",
        "\n",
        "def main(file: str, num_files: int = -1):\n",
        "    # Use multiprocessing to work in parallel\n",
        "    count = multiprocessing.cpu_count()\n",
        "    with multiprocessing.Pool(processes=count) as pool:\n",
        "        hashes = parseHashes(file)[:num_files]\n",
        "        print(\"submitting %d jobs\" % len(hashes))\n",
        "        job_ids = pool.map(submitJob, hashes)\n",
        "        assert len(job_ids) == len(hashes)\n",
        "\n",
        "        print(\"waiting for jobs to complete...\")\n",
        "        while True:\n",
        "            job_statuses = pool.map(checkStatusOfJob, job_ids)\n",
        "            total_finished = sum(map(lambda x: x == \"Completed\", job_statuses))\n",
        "            if total_finished >= len(job_ids):\n",
        "                break\n",
        "            print(\"%d/%d jobs completed\" % (total_finished, len(job_ids)))\n",
        "            time.sleep(2)\n",
        "\n",
        "        print(\"all jobs completed, saving results...\")\n",
        "        results = pool.map(getResultsFromJob, job_ids)\n",
        "        print(\"finished saving results\")\n",
        "\n",
        "        # Do something with the results\n",
        "        shutil.rmtree(\"results\", ignore_errors=True)\n",
        "        os.makedirs(\"results\", exist_ok=True)\n",
        "        for r in results:\n",
        "            path = os.path.join(r, \"combined_results\", \"outputs\", \"*.csv\")\n",
        "            csv_file = glob.glob(path)\n",
        "            for f in csv_file:\n",
        "                print(\"moving %s to results\" % f)\n",
        "                shutil.move(f, \"results\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(\"hashes.txt\", 10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code has a few interesting features:\n",
        "* Change the value in the `main` call to change the number of jobs to execute\n",
        "* Because all jobs complete at different times, there's a loop to check that all jobs have completed before downloading the results -- if you don't do this you'll likely see an error when trying to download the results\n",
        "* When downloading the results, the IPFS get often times out, so I wrapped that in a loop\n",
        "\n",
        "Let's run it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "submitting 10 jobs\n",
            "job submitted: 239f89b9-c9c2-4b24-b314-030f20d2ed86\n",
            "job submitted: 5ce48688-fe02-4514-a9d0-a16f43634e8a\n",
            "job submitted: 1ab7f06c-4c50-4617-8994-1904aa3bb1de\n",
            "job submitted: 04201261-467b-4985-9b25-dacd59a2b6ab\n",
            "job submitted: c2d181fd-ec3a-4320-abe2-01dd66784b5f\n",
            "job submitted: 99a1d838-4ecc-45aa-ba02-67afcf4eaa4c\n",
            "job submitted: 8d1bfc74-d623-45af-866d-d8d3100889e0\n",
            "job submitted: ff777c33-04c5-43ca-a291-397c6e11e297\n",
            "job submitted: 8cbfffa4-aef7-4413-9042-6776d5eddc33\n",
            "job submitted: 50f0a7f1-6619-4b7b-ad53-65858389a8cd\n",
            "waiting for jobs to complete...\n",
            "job not completed: 8d1bfc74-d623-45af-866d-d8d3100889e0 - Waiting\n",
            "job not completed: 1ab7f06c-4c50-4617-8994-1904aa3bb1de - Waiting\n",
            "job not completed: 5ce48688-fe02-4514-a9d0-a16f43634e8a - Waiting\n",
            "job not completed: 04201261-467b-4985-9b25-dacd59a2b6ab - Waiting\n",
            "job not completed: ff777c33-04c5-43ca-a291-397c6e11e297 - Waiting\n",
            "job not completed: 99a1d838-4ecc-45aa-ba02-67afcf4eaa4c - Waiting\n",
            "job not completed: c2d181fd-ec3a-4320-abe2-01dd66784b5f - Waiting\n",
            "job not completed: 239f89b9-c9c2-4b24-b314-030f20d2ed86 - Waiting\n",
            "job not completed: 50f0a7f1-6619-4b7b-ad53-65858389a8cd - Waiting\n",
            "job not completed: 8cbfffa4-aef7-4413-9042-6776d5eddc33 - Waiting\n",
            "0/10 jobs completed\n",
            "job not completed: ff777c33-04c5-43ca-a291-397c6e11e297 - Waiting\n",
            "job not completed: 04201261-467b-4985-9b25-dacd59a2b6ab - Waiting\n",
            "job completed: 239f89b9-c9c2-4b24-b314-030f20d2ed86\n",
            "job not completed: 1ab7f06c-4c50-4617-8994-1904aa3bb1de - Waiting\n",
            "job not completed: c2d181fd-ec3a-4320-abe2-01dd66784b5f - Waiting\n",
            "job not completed: 8d1bfc74-d623-45af-866d-d8d3100889e0 - Waiting\n",
            "job completed: 99a1d838-4ecc-45aa-ba02-67afcf4eaa4c\n",
            "job completed: 5ce48688-fe02-4514-a9d0-a16f43634e8a\n",
            "job not completed: 50f0a7f1-6619-4b7b-ad53-65858389a8cd - Waiting\n",
            "job not completed: 8cbfffa4-aef7-4413-9042-6776d5eddc33 - Waiting\n",
            "3/10 jobs completed\n",
            "job completed: 5ce48688-fe02-4514-a9d0-a16f43634e8a\n",
            "job completed: c2d181fd-ec3a-4320-abe2-01dd66784b5f\n",
            "job completed: 04201261-467b-4985-9b25-dacd59a2b6ab\n",
            "job completed: 8d1bfc74-d623-45af-866d-d8d3100889e0\n",
            "job completed: ff777c33-04c5-43ca-a291-397c6e11e297\n",
            "job completed: 239f89b9-c9c2-4b24-b314-030f20d2ed86\n",
            "job completed: 99a1d838-4ecc-45aa-ba02-67afcf4eaa4c\n",
            "job completed: 1ab7f06c-4c50-4617-8994-1904aa3bb1de\n",
            "job completed: 8cbfffa4-aef7-4413-9042-6776d5eddc33\n",
            "job completed: 50f0a7f1-6619-4b7b-ad53-65858389a8cd\n",
            "all jobs completed, saving results...\n",
            "getting results for job: c2d181fd-ec3a-4320-abe2-01dd66784b5f\n",
            "getting results for job: 239f89b9-c9c2-4b24-b314-030f20d2ed86\n",
            "getting results for job: 99a1d838-4ecc-45aa-ba02-67afcf4eaa4c\n",
            "getting results for job: ff777c33-04c5-43ca-a291-397c6e11e297\n",
            "getting results for job: 5ce48688-fe02-4514-a9d0-a16f43634e8a\n",
            "getting results for job: 04201261-467b-4985-9b25-dacd59a2b6ab\n",
            "getting results for job: 8d1bfc74-d623-45af-866d-d8d3100889e0\n",
            "getting results for job: 1ab7f06c-4c50-4617-8994-1904aa3bb1de\n",
            "getting results for job: 50f0a7f1-6619-4b7b-ad53-65858389a8cd\n",
            "getting results for job: 8cbfffa4-aef7-4413-9042-6776d5eddc33\n",
            "finished saving results\n",
            "moving /var/folders/kr/pl4p96k11b55hp5_p9l_t8kr0000gn/T/tmpui2gdnta/combined_results/outputs/transactions_00000000_00049999.csv to results\n",
            "moving /var/folders/kr/pl4p96k11b55hp5_p9l_t8kr0000gn/T/tmphktn81xx/combined_results/outputs/transactions_00050000_00099999.csv to results\n",
            "moving /var/folders/kr/pl4p96k11b55hp5_p9l_t8kr0000gn/T/tmp15x2meyr/combined_results/outputs/transactions_00100000_00149999.csv to results\n",
            "moving /var/folders/kr/pl4p96k11b55hp5_p9l_t8kr0000gn/T/tmp7md5a9yc/combined_results/outputs/transactions_00150000_00199999.csv to results\n",
            "moving /var/folders/kr/pl4p96k11b55hp5_p9l_t8kr0000gn/T/tmpf_3mxm23/combined_results/outputs/transactions_00200000_00249999.csv to results\n",
            "moving /var/folders/kr/pl4p96k11b55hp5_p9l_t8kr0000gn/T/tmpgybx5jqy/combined_results/outputs/transactions_00250000_00299999.csv to results\n",
            "moving /var/folders/kr/pl4p96k11b55hp5_p9l_t8kr0000gn/T/tmpsvdyyig7/combined_results/outputs/transactions_00300000_00349999.csv to results\n",
            "moving /var/folders/kr/pl4p96k11b55hp5_p9l_t8kr0000gn/T/tmpsjyrsujx/combined_results/outputs/transactions_00350000_00399999.csv to results\n",
            "moving /var/folders/kr/pl4p96k11b55hp5_p9l_t8kr0000gn/T/tmpo76qn5fi/combined_results/outputs/transactions_00400000_00449999.csv to results\n",
            "moving /var/folders/kr/pl4p96k11b55hp5_p9l_t8kr0000gn/T/tmp3xoyumpj/combined_results/outputs/transactions_00450000_00499999.csv to results\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "python bacalhau.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hopefully the results directory contains all the combined results from the jobs we just executed. Here's we're expecting to see csv files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 40\n",
            "-rw-r--r-- 3 phil staff  55 Nov 29 11:16 transactions_00000000_00049999.csv\n",
            "-rw-r--r-- 3 phil staff 387 Nov 29 11:17 transactions_00050000_00099999.csv\n",
            "-rw-r--r-- 3 phil staff 388 Nov 29 11:17 transactions_00100000_00149999.csv\n",
            "-rw-r--r-- 3 phil staff 426 Nov 29 11:16 transactions_00150000_00199999.csv\n",
            "-rw-r--r-- 3 phil staff 393 Nov 29 11:17 transactions_00200000_00249999.csv\n",
            "-rw-r--r-- 3 phil staff 384 Nov 29 11:16 transactions_00250000_00299999.csv\n",
            "-rw-r--r-- 3 phil staff 421 Nov 29 11:16 transactions_00300000_00349999.csv\n",
            "-rw-r--r-- 3 phil staff 390 Nov 29 11:17 transactions_00350000_00399999.csv\n",
            "-rw-r--r-- 3 phil staff 347 Nov 29 11:17 transactions_00400000_00449999.csv\n",
            "-rw-r--r-- 3 phil staff 386 Nov 29 11:18 transactions_00450000_00499999.csv\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "ls -l results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Success! We've now executed a bunch of jobs in parallel using Python. This is a great way to execute lots of jobs in a repeatable manner. You can alter the file above for your purposes.\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "You might also be interested in the following examples:\n",
        "\n",
        "* [Analysing Ethereum Data with Python](../../data-engineering/blockchain-etl/index.md)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPMkD4lMPwZFqll4/eh5uBe",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit ('3.9.7')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "21fd917facdca5c02b7d24e32528f1b4e6711465b0262edbfffba943391e1222"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
