{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "tags": []
      },
      "source": [
        "---\n",
        "sidebar_label: \"Python - Pandas\"\n",
        "sidebar_position: 2\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JcAzJVmEau1",
        "tags": []
      },
      "source": [
        "# Running Pandas on Bacalhau\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bacalhau-project/examples/blob/main/workload-onboarding/python-pandas/index.ipynb)\n",
        "[![Open In Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/bacalhau-project/examples/HEAD?labpath=workload-onboarding/python-pandas/index.ipynb)\n",
        "\n",
        "### Introduction\n",
        "\n",
        "Pandas is a Python package that provides fast, flexible, and expressive data structures designed to make working with data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real-world data analysis in Python. Additionally, it has the broader goal of becoming the most powerful and flexible open source data analysis/manipulation tool available in any language. It is already well on its way towards this goal.\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "* Python\n",
        "* The Bacalhau client - [Installation instructions](https://docs.bacalhau.org/getting-started/installation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "remove_cell"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: PATH=./:/Users/phil/.cargo/bin:/Users/phil/.pyenv/versions/3.9.7/bin:/opt/homebrew/Caskroom/google-cloud-sdk/latest/google-cloud-sdk/bin:/Users/phil/.gvm/bin:/opt/homebrew/opt/findutils/libexec/gnubin:/opt/homebrew/opt/coreutils/libexec/gnubin:/opt/homebrew/Caskroom/google-cloud-sdk/latest/google-cloud-sdk/bin:/Users/phil/.pyenv/shims:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/usr/local/MacGPG2/bin:/Users/phil/.nexustools\n"
          ]
        }
      ],
      "source": [
        "!command -v bacalhau >/dev/null 2>&1 || (export BACALHAU_INSTALL_DIR=.; curl -sL https://get.bacalhau.org/install.sh | bash)\n",
        "path=!echo $PATH\n",
        "%env PATH=./:{path[0]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 1. Getting Started with Pandas Locally\n",
        "\n",
        "The goal of this section is to show you how to develop a script to perform a task. We will then place this script in a container and run it at scale on Bacalhau. But first, you will need to install the Pandas library from pip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iD_DesnwEodT",
        "tags": [
          "remove_output"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /Users/phil/.pyenv/versions/3.9.7/lib/python3.9/site-packages (1.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/phil/.local/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /Users/phil/.pyenv/versions/3.9.7/lib/python3.9/site-packages (from pandas) (1.23.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/phil/.local/lib/python3.9/site-packages (from pandas) (2021.1)\n",
            "Requirement already satisfied: six>=1.5 in /Users/phil/.pyenv/versions/3.9.7/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWsoWlxIEwPD",
        "tags": []
      },
      "source": [
        "### Importing data from CSV to DataFrame\n",
        "\n",
        "Pandas is built around the idea of a DataFrame, a container for representing data. Below you will create a DataFrame by importing a CSV file. A CSV file is a text file with one record of data per line. The values within the record are separated using the “comma” character. Pandas provides a useful method, named `read_csv()` to read the contents of the CSV file into a DataFrame. For example, we can create a file named `transactions.csv` containing details of Transactions. The CSV file is stored in the same directory that contains Python script.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1217,
          "status": "ok",
          "timestamp": 1663261247850,
          "user": {
            "displayName": "Vedant Padwal",
            "userId": "05534807475078682627"
          },
          "user_tz": -330
        },
        "id": "t5tks1xQFMja",
        "outputId": "46aaf8a9-991f-429e-aae4-57fbd7ae7317",
        "scrolled": false,
        "tags": [
          "remove_output"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting read_csv.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile read_csv.py\n",
        "import pandas as pd\n",
        "\n",
        "print(pd.read_csv(\"transactions.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1285,
          "status": "ok",
          "timestamp": 1663261259098,
          "user": {
            "displayName": "Vedant Padwal",
            "userId": "05534807475078682627"
          },
          "user_tz": -330
        },
        "id": "GFfvbdL3E18R",
        "outputId": "e53a32c6-86e4-4649-f6f8-ab4eb0d2d71e",
        "tags": [
          "remove_output"
        ]
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "--2022-11-23 13:28:29--  https://cloudflare-ipfs.com/ipfs/QmfKJT13h5k1b23ja3ZCVg5nFL9oKz2bVXc8oXgtwiwhjz/transactions.csv\n",
            "Resolving cloudflare-ipfs.com (cloudflare-ipfs.com)... 104.17.64.14, 104.17.96.13\n",
            "Connecting to cloudflare-ipfs.com (cloudflare-ipfs.com)|104.17.64.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1567 (1.5K) [text/csv]\n",
            "Saving to: ‘transactions.csv.1’\n",
            "\n",
            "     0K .                                                     100% 9.64M=0s\n",
            "\n",
            "2022-11-23 13:28:29 (9.64 MB/s) - ‘transactions.csv.1’ saved [1567/1567]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# Downloading the dataset\n",
        "wget https://cloudflare-ipfs.com/ipfs/QmfKJT13h5k1b23ja3ZCVg5nFL9oKz2bVXc8oXgtwiwhjz/transactions.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 6,
          "status": "ok",
          "timestamp": 1663261266498,
          "user": {
            "displayName": "Vedant Padwal",
            "userId": "05534807475078682627"
          },
          "user_tz": -330
        },
        "id": "U3BBfV7CFd0a",
        "outputId": "5a0248df-fadf-447e-b9e3-9bd07b26e0e0",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hash,nonce,block_hash,block_number,transaction_index,from_address,to_address,value,gas,gas_price,input,block_timestamp,max_fee_per_gas,max_priority_fee_per_gas,transaction_type\n",
            "0x04cbcb236043d8fb7839e07bbc7f5eed692fb2ca55d897f1101eac3e3ad4fab8,12,0x246edb4b351d93c27926f4649bcf6c24366e2a7c7c718dc9158eea20c03bc6ae,483920,0,0x1b63142628311395ceafeea5667e7c9026c862ca,0xf4eced2f682ce333f96f2d8966c613ded8fc95dd,0,150853,50000000000,0xa9059cbb000000000000000000000000ac4df82fe37ea2187bc8c011a23d743b4f39019a00000000000000000000000000000000000000000000000000000000000186a0,1446561880,,,0\n",
            "0xcea6f89720cc1d2f46cc7a935463ae0b99dd5fad9c91bb7357de5421511cee49,84,0x246edb4b351d93c27926f4649bcf6c24366e2a7c7c718dc9158eea20c03bc6ae,483920,1,0x9b22a80d5c7b3374a05b446081f97d0a34079e7f,0xf4eced2f682ce333f96f2d8966c613ded8fc95dd,0,150853,50000000000,0xa9059cbb00000000000000000000000066f183060253cfbe45beff1e6e7ebbe318c81e560000000000000000000000000000000000000000000000000000000000030d40,1446561880,,,0\n",
            "0x463d53f0ad57677a3b430a007c1c31d15d62c37fab5eee598551697c297c235c,88,0x246edb4b351d93c27926f4649bcf6c24366e2a7c7c718dc9158eea20c03bc6ae,483920,2,0x9df428a91ff0f3635c8f0ce752933b9788926804,0x9e669f970ec0f49bb735f20799a7e7c4a1c274e2,11000440000000000,90000,50000000000,0x,1446561880,,,0\n",
            "0x05287a561f218418892ab053adfb3d919860988b19458c570c5c30f51c146f02,20085,0x246edb4b351d93c27926f4649bcf6c24366e2a7c7c718dc9158eea20c03bc6ae,483920,3,0x2a65aca4d5fc5b5c859090a6c34d164135398226,0x743b8aeedc163c0e3a0fe9f3910d146c48e70da8,1530219620000000000,90000,50000000000,0x,1446561880,,,0"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "cat transactions.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmiQMOBwGaY5",
        "tags": []
      },
      "source": [
        "### Running the script\n",
        "\n",
        "Now let's run the script to read in the CSV file. The output will be a DataFrame object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 957,
          "status": "ok",
          "timestamp": 1663261342062,
          "user": {
            "displayName": "Vedant Padwal",
            "userId": "05534807475078682627"
          },
          "user_tz": -330
        },
        "id": "R7Y0G8L_GMse",
        "outputId": "e151de05-7acd-43b5-c79b-5b0831f23232",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                hash  ...  transaction_type\n",
            "0  0x04cbcb236043d8fb7839e07bbc7f5eed692fb2ca55d8...  ...                 0\n",
            "1  0xcea6f89720cc1d2f46cc7a935463ae0b99dd5fad9c91...  ...                 0\n",
            "2  0x463d53f0ad57677a3b430a007c1c31d15d62c37fab5e...  ...                 0\n",
            "3  0x05287a561f218418892ab053adfb3d919860988b1945...  ...                 0\n",
            "\n",
            "[4 rows x 15 columns]\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "python3 read_csv.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gcvny737Giog",
        "tags": []
      },
      "source": [
        "## 2. Running Pandas Jobs At Scale on Bacalhau\n",
        "\n",
        "To run pandas on Bacalhau you must store your assets in a location that Bacalhau has access to. We usually default to storing data on IPFS and code in a container, but you can also easily upload your script to IPFS too.\n",
        "\n",
        "If you are interested in finding out more about how to ingest your data into IPFS, please see the [data ingestion guide](../../data-ingestion/index.md).\n",
        "\n",
        "We've already uploaded the script and data to IPFS to the following CID: `QmfKJT13h5k1b23ja3ZCVg5nFL9oKz2bVXc8oXgtwiwhjz`. You can look at this by browsing to one of the HTTP IPFS proxies like [ipfs.io](https://cloudflare-ipfs.com/ipfs/QmfKJT13h5k1b23ja3ZCVg5nFL9oKz2bVXc8oXgtwiwhjz/) or [w3s.link](https://bafybeih4hyydvojazlyv5zseelgn5u67iq2wbrbk2q4xoiw2d3cacdmzlu.ipfs.w3s.link/).\n",
        "\n",
        "### Run the Job\n",
        "\n",
        "Now we're ready to run a Bacalhau job, whilst mounting the Pandas script and data from IPFS. We'll use the `bacalhau docker run` command to do this. The `-v` flag allows us to mount a file or directory from IPFS into the container. The `-v` flag takes two arguments, the first is the IPFS CID and the second is the path to the directory in the container. The `-v` flag can be used multiple times to mount multiple directories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 653,
          "status": "ok",
          "timestamp": 1663261638758,
          "user": {
            "displayName": "Vedant Padwal",
            "userId": "05534807475078682627"
          },
          "user_tz": -330
        },
        "id": "uzs8k0CMGfJL",
        "outputId": "4f5d7a8e-98ee-4046-a1b3-ef221a0c6708",
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%bash --out job_id\n",
        " bacalhau docker run \\\n",
        "--wait \\\n",
        "--id-only \\\n",
        "-v QmfKJT13h5k1b23ja3ZCVg5nFL9oKz2bVXc8oXgtwiwhjz:/files \\\n",
        "-w /files \\\n",
        "amancevice/pandas \\\n",
        "-- python read_csv.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "tags": [
          "remove_cell"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: JOB_ID=d48079d4-1358-4ce1-8a9e-5b9e6ae40bda\n"
          ]
        }
      ],
      "source": [
        "%env JOB_ID={job_id}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opCdYFjaHrQP",
        "tags": []
      },
      "source": [
        "Running the commands will output a UUID (like `e6377c99-b637-4661-a334-6ce98fcf037c`). This is the ID of the job that was created. You can check the status of the job with the following command:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 2473,
          "status": "ok",
          "timestamp": 1663261688339,
          "user": {
            "displayName": "Vedant Padwal",
            "userId": "05534807475078682627"
          },
          "user_tz": -330
        },
        "id": "MmLc6twCHngK",
        "outputId": "48493fed-2b8b-4085-b534-a7091bec79e4",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[92;100m CREATED  \u001b[0m\u001b[92;100m ID       \u001b[0m\u001b[92;100m JOB                     \u001b[0m\u001b[92;100m STATE     \u001b[0m\u001b[92;100m VERIFIED \u001b[0m\u001b[92;100m PUBLISHED               \u001b[0m\n",
            "\u001b[97;40m 13:38:11 \u001b[0m\u001b[97;40m d48079d4 \u001b[0m\u001b[97;40m Docker amancevice/pa... \u001b[0m\u001b[97;40m Completed \u001b[0m\u001b[97;40m          \u001b[0m\u001b[97;40m /ipfs/QmY2MEETWyX77B... \u001b[0m\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "bacalhau list --id-filter ${JOB_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mO8wG4XMH5eA",
        "tags": []
      },
      "source": [
        "\n",
        "Where it says \"`Published`\", that means the job is done, and we can get the results.\n",
        "\n",
        "If there is an error you can view the error using the following command bacalhau describe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GlzbQutAHzKi",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "APIVersion: V1beta1\n",
            "ClientID: 77cf46c04f88ffb1c3e0e4b6e443724e8d2d87074d088ef1a6294a448fa85d2e\n",
            "CreatedAt: \"2022-11-23T13:38:11.136995358Z\"\n",
            "Deal:\n",
            "  Concurrency: 1\n",
            "ExecutionPlan:\n",
            "  ShardsTotal: 1\n",
            "ID: d48079d4-1358-4ce1-8a9e-5b9e6ae40bda\n",
            "JobState:\n",
            "  Nodes:\n",
            "    QmSyJ8VUd4YSPwZFJSJsHmmmmg7sd4BAc2yHY73nisJo86:\n",
            "      Shards:\n",
            "        \"0\":\n",
            "          NodeId: QmSyJ8VUd4YSPwZFJSJsHmmmmg7sd4BAc2yHY73nisJo86\n",
            "          PublishedResults: {}\n",
            "          State: Cancelled\n",
            "          VerificationResult: {}\n",
            "    QmXaXu9N5GNetatsvwnTfQqNtSeKAD6uCmarbh3LMRYAcF:\n",
            "      Shards:\n",
            "        \"0\":\n",
            "          NodeId: QmXaXu9N5GNetatsvwnTfQqNtSeKAD6uCmarbh3LMRYAcF\n",
            "          PublishedResults:\n",
            "            CID: QmY2MEETWyX77BBYBNBpUW5bjkVAyP87EotPDVW2vjHG8K\n",
            "            Name: job-d48079d4-1358-4ce1-8a9e-5b9e6ae40bda-shard-0-host-QmXaXu9N5GNetatsvwnTfQqNtSeKAD6uCmarbh3LMRYAcF\n",
            "            StorageSource: IPFS\n",
            "          RunOutput:\n",
            "            exitCode: 0\n",
            "            runnerError: \"\"\n",
            "            stderr: \"\"\n",
            "            stderrtruncated: false\n",
            "            stdout: |2\n",
            "                                                              hash  ...  transaction_type\n",
            "              0  0x04cbcb236043d8fb7839e07bbc7f5eed692fb2ca55d8...  ...                 0\n",
            "              1  0xcea6f89720cc1d2f46cc7a935463ae0b99dd5fad9c91...  ...                 0\n",
            "              2  0x463d53f0ad57677a3b430a007c1c31d15d62c37fab5e...  ...                 0\n",
            "              3  0x05287a561f218418892ab053adfb3d919860988b1945...  ...                 0\n",
            "\n",
            "              [4 rows x 15 columns]\n",
            "            stdouttruncated: false\n",
            "          State: Completed\n",
            "          Status: 'Got results proposal of length: 0'\n",
            "          VerificationResult:\n",
            "            Complete: true\n",
            "            Result: true\n",
            "RequesterNodeID: QmXaXu9N5GNetatsvwnTfQqNtSeKAD6uCmarbh3LMRYAcF\n",
            "RequesterPublicKey: CAASpgIwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCehDIWl72XKJi1tsrYM9JjAWt3n6hNzrCA+IVRXixK1sJVTLMpsxEP8UKJI+koAWkAUuY8yi6DMzot0owK4VpM3PYp34HdKi2hTjzM8pjCVb70XVXt6k9bzj4KmbiQTuEkQfvwIRmgxb2jrkRdTpZmhMb1Q7StR/nrGa/bx75Vpupx1EYH6+LixYnnV5WbCUK/kjpBW8SF5v+f9ZO61KHd9DMpdhJnzocTGq17tAjHh3birke0xlP98JjxlMkzzvIAuFsnH0zBIgjmHDA1Yi5DcOPWgE0jUfGlSDC1t2xITVoofHQcXDjkHZE6OhxswNYPd7cnTf9OppLddFdQnga5AgMBAAE=\n",
            "Spec:\n",
            "  Docker:\n",
            "    Entrypoint:\n",
            "    - python\n",
            "    - read_csv.py\n",
            "    Image: amancevice/pandas\n",
            "    WorkingDirectory: /files\n",
            "  Engine: Docker\n",
            "  Language:\n",
            "    JobContext: {}\n",
            "  Publisher: Estuary\n",
            "  Resources:\n",
            "    GPU: \"\"\n",
            "  Sharding:\n",
            "    BatchSize: 1\n",
            "    GlobPatternBasePath: /inputs\n",
            "  Timeout: 1800\n",
            "  Verifier: Noop\n",
            "  Wasm: {}\n",
            "  inputs:\n",
            "  - CID: QmfKJT13h5k1b23ja3ZCVg5nFL9oKz2bVXc8oXgtwiwhjz\n",
            "    StorageSource: IPFS\n",
            "    path: /files\n",
            "  outputs:\n",
            "  - Name: outputs\n",
            "    StorageSource: IPFS\n",
            "    path: /outputs\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "bacalhau describe ${JOB_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSEblyHIIB4T",
        "tags": []
      },
      "source": [
        "The describe command will display the logs and error messages from your job. There's no errors this time (lucky?) so now let's create a temporary directory to save our results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Y06KRWLDIitN",
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "rm -rf results && mkdir -p results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N_XsPABIncF",
        "tags": []
      },
      "source": [
        "To Download the results of your job, run the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 16481,
          "status": "ok",
          "timestamp": 1663262062088,
          "user": {
            "displayName": "Vedant Padwal",
            "userId": "05534807475078682627"
          },
          "user_tz": -330
        },
        "id": "iHl4-fa3H-Yt",
        "outputId": "fcfdd0e0-ad3d-4afa-b771-dea1c4b3841c",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching results of job 'd48079d4-1358-4ce1-8a9e-5b9e6ae40bda'...\n",
            "Results for job 'd48079d4-1358-4ce1-8a9e-5b9e6ae40bda' have been written to...\n",
            "results\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "bacalhau get ${JOB_ID}  --output-dir results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdU_la6dJTsf",
        "tags": []
      },
      "source": [
        "After the download has finished you should \n",
        "see the following contents in pandas-results directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 5,
          "status": "ok",
          "timestamp": 1663262118236,
          "user": {
            "displayName": "Vedant Padwal",
            "userId": "05534807475078682627"
          },
          "user_tz": -330
        },
        "id": "bpbTMhO3INLw",
        "outputId": "a7462902-9895-418d-c123-da561a5b0d1e",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "outputs\n",
            "stderr\n",
            "stdout\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "ls results/combined_results/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_q10kBMJjch",
        "tags": []
      },
      "source": [
        "The structure of the files and directories will look like this:\n",
        "\n",
        "```\n",
        ".\n",
        "├── combined_results\n",
        "│   ├── outputs\n",
        "│   ├── stderr\n",
        "│   └── stdout\n",
        "├── per_shard\n",
        "│   └── 0_node_QmSyJ8VU\n",
        "│       ├── exitCode\n",
        "│       ├── outputs\n",
        "│       ├── stderr\n",
        "│       └── stdout\n",
        "└── raw\n",
        "    └── QmY2MEETWyX77BBYBNBpUW5bjkVAyP87EotPDVW2vjHG8K\n",
        "        ├── exitCode\n",
        "        ├── outputs\n",
        "        ├── stderr\n",
        "        └── stdout\n",
        "```\n",
        "\n",
        "* `stdout` contains things printed to the console like outputs, etc.\n",
        "\n",
        "* `stderr` contains any errors. In this case, since there are no errors, it's will be empty\n",
        "\n",
        "* `outputs` folder is the volume you named when you started the job with the `-o` flag. In addition, you will always have a `outputs` volume, which is provided by default.\n",
        "\n",
        "Because your script is printed to stdout, the output will appear in the stdout file. You can read this by typing the following command:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 7,
          "status": "ok",
          "timestamp": 1663262342563,
          "user": {
            "displayName": "Vedant Padwal",
            "userId": "05534807475078682627"
          },
          "user_tz": -330
        },
        "id": "K8Od6pahJw5J",
        "outputId": "5f0a8cd8-2ee7-4c05-f106-bc80c5267811",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                hash  ...  transaction_type\n",
            "0  0x04cbcb236043d8fb7839e07bbc7f5eed692fb2ca55d8...  ...                 0\n",
            "1  0xcea6f89720cc1d2f46cc7a935463ae0b99dd5fad9c91...  ...                 0\n",
            "2  0x463d53f0ad57677a3b430a007c1c31d15d62c37fab5e...  ...                 0\n",
            "3  0x05287a561f218418892ab053adfb3d919860988b1945...  ...                 0\n",
            "\n",
            "[4 rows x 15 columns]\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "cat results/combined_results/stdout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Success! The next step is to scale up your data and your processing via multiple jobs or sharding. You might be interested in looking at:\n",
        "\n",
        "* [An example running hundreds of jobs over \"big data\"](../../data-engineering/blockchain-etl/index.md)\n",
        "* [A simple sharding example](../../data-engineering/simple-parallel-workloads/index.md)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPMkD4lMPwZFqll4/eh5uBe",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit ('3.9.7')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "21fd917facdca5c02b7d24e32528f1b4e6711465b0262edbfffba943391e1222"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
