input {
  file {
    path => "/file.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    codec => "plain"
    tags => ["raw_log"]
  }
}

filter {
  grok {
    match => {
      "message" => "%{IP:remote_addr} - %{DATA:remote_user} \[%{HTTPDATE:time_local}\] \"%{WORD:http_method} %{DATA:request} HTTP/%{NUMBER:http_version}\" %{NUMBER:status} %{NUMBER:body_bytes_sent} \"%{DATA:http_referer}\" \"%{DATA:http_user_agent}\""
    }
  }

  date {
    match => [ "time_local", "dd/MMM/yyyy:HH:mm:ss Z" ]
    target => "@timestamp"
  }

  # Clone the event to be able to delive the raw log to S3, and the aggregated log to Elasticsearch
  clone {
    clones => ["raw_log"]
    add_tag => ["_aggregate"]
  }

  # Aggregate the logs
  if "_aggregate" in [tags] {

    # Drop the raw log tag
    mutate {
      remove_tag => ["raw_log"]
    }

    # Add geoip information
    geoip {
      source => "remote_addr"
      target => "geoip"
      fields => ["LOCATION", "COUNTRY_CODE2", "CITY_NAME"]
    }

    useragent {
      source => 'http_user_agent'
      target => 'user_agent'
    }
    
    # Normalize request paths to enhance aggregation
    ruby {
      path => "/logstash/filters/normalize_request.rb"
    }

    # Aggregate the data
    ruby {
      path => "/logstash/filters/aggregate.rb"
      script_params => { "duration" => 30}
    }


    # Sort the aggregated data and only keep the top 10 of each attribute
    ruby {
      path => "/logstash/filters/sort_and_output.rb"
      script_params => { 
        "min_count" => 3
        "limit" => 10 
      }
    }

  }
}

output {
  if "_aggregated_stats" in [tags] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "logstash-aggregated-stats-%{+YYYY.MM.dd}"
    }
    
    # stdout {
    #   codec => rubydebug
    # }
  } else if "_aggregated_geoip" in [tags] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "logstash-aggregated-geoip-%{+YYYY.MM.dd}"
    }
    
    # stdout {
    #   codec => rubydebug
    # }
  } else if "raw_log" in [tags] {
    s3 {
      access_key_id => ""
      secret_access_key => ""
      region => "eu-west-1"
      bucket => "bacalhau-usecase-log-vending"
      prefix => "%{+YYYY}-%{+MM}-%{+dd}"
      time_file => 60
      rotation_strategy => "time"
      encoding => "gzip"
    }
  }
}



